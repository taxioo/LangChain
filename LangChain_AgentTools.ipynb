{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ee22af",
   "metadata": {},
   "source": [
    "# Agent & Tools\n",
    "\n",
    "## 도구 호출이란?\n",
    "도구 호출(Tool Calling)은 LangChain에서 언어 모델이 외부 도구나 API를 사용할 수 있게 해주는 중요한 기능입니다. \n",
    "이를 통해 AI 모델은 실시간 정보 검색, 계산, 데이터베이스 쿼리 등 다양한 작업을 수행할 수 있습니다. \n",
    "특히, 에이전트(Agent) 기반의 시스템을 만들 때 매우 중요한 역할을 하기 때문에 잘 알아둘 필요가 있습니다.\n",
    "\n",
    "## 도구 호출의 중요성\n",
    "1. 기능 확장\n",
    "   1. 언어 모델의 능력을 크게 확장시킵니다. 모델은 자체적으로 가지고 있지 않은 정보나 기능에 접근할 수 있습니다.\n",
    "2. 실시간 정보 접근\n",
    "   1. 웹 검색 도구를 통해 최신 정보를 얻을 수 있어, 항상 최신의 답변을 제공할 수 있습니다.\n",
    "3. 정확성 향상\n",
    "   1. 계산기 도구나 데이터베이스 쿼리 도구를 사용하여 정확한 수치나 데이터를 얻을 수 있습니다.\n",
    "4. 다양한 작업 수행\n",
    "   1. 코드 실행, 파일 조작, API 호출 등 다양한 작업을 수행할 수 있습니다.\n",
    "\n",
    "## LangChain v1.0의 Agent 시스템\n",
    "### 새로운 특징\n",
    "1. 통합된 create_agent() API\n",
    "   1. 이전 버전 \n",
    "      1. initialize_agent() 함수와 복잡한 AgentType enum 사용\n",
    "   2. 현재 버전 \n",
    "      1. create_agent() 통합 API로 단순화 - AgentType enum 제거 - 더 직관적인 파라미터 구조\n",
    "하나의 함수로 모든 타입의 에이전트를 생성할 수 있습니다.\n",
    "\n",
    "2. 미들웨어 시스템\n",
    "에이전트의 동작을 세밀하게 제어할 수 있는 미들웨어 시스템이 추가되었습니다.\n",
    "- dynamic_prompt: 런타임에 동적으로 시스템 프롬프트 생성\n",
    "- wrap_tool_call: 도구 호출을 래핑하여 에러 처리 및 로깅\n",
    "- wrap_model_call: 모델 호출을 인터셉트하여 커스텀 로직 추가\n",
    "\n",
    "3. 간결한 프롬프트 설정\n",
    "system_prompt 파라미터로 프롬프트를 직접 설정할 수 있습니다.\n",
    "\n",
    "4. TypedDict 기반 상태 스키마\n",
    "타입 안정성이 강화되어 더 안전한 코드 작성이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8dfbc9",
   "metadata": {},
   "source": [
    "- 도구 호출의 작동 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeeda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 도구 정의\n",
    "# 개발자가 사용할 도구들을 정의하고 설정합니다.\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    # 웹 검색 로직\n",
    "    return \"검색 결과...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117397c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 에이전트 생성\n",
    "# 정의한 도구들을 에이전트에 등록합니다.\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search_web],\n",
    "    system_prompt=\"You are a helpful assistant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb48117",
   "metadata": {},
   "source": [
    "3. 모델 추론\n",
    "언어 모델이 주어진 태스크를 분석하고 필요한 도구를 결정합니다.\n",
    "\n",
    "4. 도구 호출\n",
    "모델이 선택한 도구를 호출하고 결과를 받아옵니다.\n",
    "\n",
    "5. 결과 통합\n",
    "도구 호출의 결과를 모델의 응답에 통합하여 최종 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "274f842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "서울\n",
      "서울의 현재 날씨는 맑고 기온은 25°C이며, 습도는 60%입니다. 좋은 하루 보내세요! ☀️\n"
     ]
    }
   ],
   "source": [
    "# 기본 예제\n",
    "# 날씨 조회 에이전트\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "\n",
    "# 1. 도구 정의\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a specific location.\n",
    "\n",
    "    Args:\n",
    "        location: The city or location name (e.g., \"Seoul\", \"New York\")\n",
    "\n",
    "    Returns:\n",
    "        Weather information as a string\n",
    "    \"\"\"\n",
    "    print(location)\n",
    "    # 실제로는 날씨 API를 호출하지만, 여기서는 예시로 고정값 반환\n",
    "    weather_data = {\n",
    "        \"Seoul\": \"맑음, 25°C, 습도 60%\",\n",
    "        \"New York\": \"흐림, 18°C, 습도 75%\",\n",
    "        \"Tokyo\": \"비, 20°C, 습도 85%\"\n",
    "    }\n",
    "    return weather_data\n",
    "\n",
    "# 2. 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"\"\"You are a helpful weather assistant.\n",
    "When users ask about weather, use the get_weather tool to fetch current information.\n",
    "Always provide the weather information in a friendly and clear manner.\"\"\"\n",
    ")\n",
    "\n",
    "# 3. 에이전트 실행\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"서울 날씨 어때?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f90cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>tool 함수에서 어떻게 location에 서울이 들어갔는가?</summary>\n",
    "\n",
    "> `location`을 `print(location)` 했을 때 “서울”이 찍힌 이유는, **에이전트(LLM)가 사용자의 자연어 질문(“서울 날씨 어때?”)에서 핵심 인자(location)를 추출해서 `get_weather` 도구를 호출할 때 그 인자로 넘겼기 때문**입니다. 즉, “서울”은 코드 어딘가에서 하드코딩되어 자동으로 들어간 게 아니라, **모델이 대화 문맥을 이해해 “이 질문의 location은 서울이네”라고 판단해서** 도구 호출을 구성한 결과예요.\n",
    "> \n",
    "> 다만, 질문에 올려주신 예제 코드는 엄밀히 말하면 몇 가지가 섞여 있습니다:\n",
    "> \n",
    "> - `langchain.agents.create_agent`는 LangChain 공식 API와 이름이 같을 수 있지만, 실제로는 **사용 중인 래퍼/버전/템플릿**에 따라 내부 동작(특히 tool calling 포맷)이 조금씩 다릅니다.\n",
    "> - 그래도 “도구 호출 → 도구 실행 → 최종 답변 생성”이라는 큰 흐름은 동일합니다.\n",
    "> \n",
    "> 아래는 **“서울 날씨 어때?” → 최종 답변**이 나오기까지의 전형적인 시나리오(내부 흐름)입니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 1) 사용자 입력이 들어옴\n",
    "> 사용자가 아래 메시지를 보냅니다.\n",
    "> \n",
    "> - User: “서울 날씨 어때?”\n",
    "> \n",
    "> 에이전트는 `invoke()`에 전달된 `messages`를 보고, system prompt도 함께 읽습니다.\n",
    "> \n",
    "> - System prompt 핵심:  \n",
    "  > “날씨 물어보면 get_weather tool을 사용해라. 친절하고 명확하게 답해라.”\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 2) 모델이 “도구를 써야겠다”고 결정\n",
    "> 모델(LLM)은 system prompt 지시 때문에 보통 이런 판단을 합니다:\n",
    "> \n",
    "> - 사용자가 “날씨”를 물었다 → 도구(get_weather)를 호출하는 게 맞다.\n",
    "> - 그럼 도구에 넣을 인자를 채워야 한다.\n",
    "> - 문장에서 도시/지역은 “서울”이다.\n",
    "> \n",
    "> 여기서 중요한 포인트는:\n",
    "> \n",
    "> - **도구 함수 시그니처가 `get_weather(location: str)`** 이기 때문에  \n",
    "  > 모델은 “location이라는 문자열 인자 하나를 채우면 되네”라고 이해합니다.\n",
    "> - 사용자의 문장 “서울 날씨 어때?”에서 **location 슬롯에 들어갈 값으로 “서울”을 추출**합니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 3) 모델이 도구 호출(tool call)을 구성\n",
    "> 이 단계에서 모델은 “최종 자연어 답변”을 바로 쓰지 않고, 내부적으로 다음과 같은 형태의 “도구 호출 요청”을 만듭니다(형식은 프레임워크/버전에 따라 다르지만 개념은 동일).\n",
    "> \n",
    "> - Tool call: `get_weather`\n",
    "> - Arguments: `{\"location\": \"서울\"}`\n",
    "> \n",
    "> 즉, **모델이 스스로 `location=\"서울\"`을 채워서 호출을 만든 것**이 “print에 서울이 찍힌” 직접 원인입니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 4) LangChain 런타임이 실제 파이썬 함수를 실행\n",
    "> 이제 에이전트 런타임(LangChain)이 모델의 도구 호출 요청을 받아서, 등록된 파이썬 함수 `get_weather`를 실행합니다.\n",
    "> \n",
    "> ```python\n",
    "> get_weather(location=\"서울\")\n",
    "> ```\n",
    "> \n",
    "> 그래서 함수 첫 줄의:\n",
    "> \n",
    "> ```python\n",
    "> print(location)\n",
    "> ```\n",
    "> \n",
    "> 이 실행되며 콘솔에\n",
    "> \n",
    "> - 서울\n",
    "> \n",
    "> 이 찍힙니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 5) 도구 함수가 결과를 반환\n",
    "> 그 다음 함수는 `weather_data`에서 값을 찾아 반환합니다.  \n",
    "> (올려주신 예제 코드에 따르면 실제로는 문자열이 아니라 dict를 반환하는 형태인데, 일단 “도구 실행 결과가 반환된다”는 흐름이 핵심입니다.)\n",
    "> \n",
    "> 예: (개념적으로)\n",
    "> - Tool result: `\"맑음, 25°C, 습도 60%\"` 또는 그에 준하는 구조화된 값\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 6) 도구 결과가 “대화 히스토리”에 다시 들어감\n",
    "> LangChain은 도구 실행 결과를 **툴 결과 메시지(tool message)**로 대화에 추가합니다.\n",
    "> \n",
    "> 대화 메시지 흐름을 단순화하면 대략 이렇게 됩니다:\n",
    "> \n",
    "> 1. system: (너는 날씨 도우미, 날씨 질문엔 tool 써라…)\n",
    "> 2. user: “서울 날씨 어때?”\n",
    "> 3. assistant: (tool call: get_weather, args: location=서울)\n",
    "> 4. tool: (result: 맑음, 25°C, 습도 60%)\n",
    "> 5. assistant: (최종 답변 생성)\n",
    "> \n",
    "> 즉, 모델은 **툴 결과를 “새로운 근거 데이터”로 받아** 마지막 응답을 작성합니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 7) 모델이 최종 자연어 답변을 생성\n",
    "> 이제 모델은 tool result를 보고 system prompt의 “friendly and clear” 지침에 맞게 문장을 자연스럽게 다듬어 최종 답변을 씁니다.\n",
    "> \n",
    "> 예를 들면:\n",
    "> \n",
    "> - “서울의 현재 날씨는 맑고 기온은 25°C이며, 습도는 60%입니다. 좋은 하루 보내세요!”\n",
    "> \n",
    "> 여기서 일어나는 일은:\n",
    "> \n",
    "> - 도구 결과(“맑음, 25°C, 습도 60%”)를 그대로 복붙하지 않고,\n",
    "> - **문장형으로 재구성(자연어 생성)** 해서 사용자에게 전달합니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> # 요약: “서울”은 어디서 왔나?\n",
    "> - 사용자의 질문에 이미 “서울”이 포함되어 있었고,\n",
    "> - LLM이 그걸 읽고 **도구 인자 `location`에 매핑해서** `get_weather(location=\"서울\")` 호출을 만들었기 때문에,\n",
    "> - 함수 안에서 `print(location)` 했을 때 “서울”이 출력된 것입니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> 원하시면, 같은 흐름을 **실제 LangChain의 tool calling 메시지 포맷(어떤 필드에 어떻게 들어가는지)** 기준으로 더 “로그처럼” 재현해드릴게요.  \n",
    "> 지금 사용 중인 `create_agent`가 **LangChain 버전/구현체가 정확히 무엇인지**(예: LCEL `create_tool_calling_agent` 계열인지, LangGraph인지, 혹은 다른 래퍼인지)만 알려주시면, 그 구현에 맞춰 더 정확한 시나리오로 써드릴 수 있어요.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f72982",
   "metadata": {},
   "source": [
    "## Agent 개요\n",
    "LangChain 1.0에서는 create_agent 함수를 통해 프로덕션 환경에서 바로 사용할 수 있는 에이전트를 제공합니다. \n",
    "이 장에서는 Agent의 개념과 create_agent API의 핵심 구성 요소를 살펴봅니다\n",
    "\n",
    "### Agent란?\n",
    "Agent(에이전트)는 언어 모델(LLM)이 도구(Tools)를 활용하여 목표를 달성하는 시스템입니다. \n",
    "에이전트는 다음과 같은 특징을 갖습니다.\n",
    "\n",
    "* 사용자의 질문을 분석하고 어떤 도구를 사용할지 스스로 결정\n",
    "* 도구 실행 결과를 바탕으로 추가 작업 수행 여부 판단\n",
    "* 최종 답변을 생성할 때까지 반복적으로 추론과 행동을 수행\n",
    "\n",
    "이러한 패턴을 ReAct(Reasoning + Acting) 패턴이라고 합니다.\n",
    "\n",
    "#### ReAct 패턴 예시\n",
    "\n",
    "`사용자`: \"최근 테슬라 주가를 검색하고 1년 수익률을 계산해줘\"\n",
    "\n",
    "`[에이전트 추론]` \"주가 정보가 필요하므로 웹 검색 도구를 사용해야겠다\"\n",
    "`[도구 실행]` search_web(\"테슬라 주가 2024\")\n",
    "`[결과 관찰]` \"현재 주가: $250, 1년 전 주가: $200\"\n",
    "\n",
    "`[에이전트 추론]` \"수익률 계산이 필요하므로 계산 도구를 사용해야겠다\"\n",
    "`[도구 실행]` calculate(\"(250-200)/200 * 100\")\n",
    "`[결과 관찰]` \"25%\"\n",
    "\n",
    "`[에이전트 추론]` \"필요한 정보를 모두 얻었으니 최종 답변을 생성하겠다\"\n",
    "`[최종 답변]` \"테슬라의 1년 수익률은 25%입니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_agent 기본 사용법\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[],  # 도구 리스트\n",
    "    system_prompt=\"당신은 유능한 어시스턴트입니다.\"\n",
    ")\n",
    "\n",
    "# 에이전트 실행\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"안녕하세요!\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f2cd6",
   "metadata": {},
   "source": [
    "* 동적 모델 선택\n",
    "  * 런타임에 상태나 컨텍스트에 따라 모델을 동적으로 선택해야 할 때는 @wrap_model_call 미들웨어를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc88c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 풀 정의\n",
    "models = {\n",
    "    \"fast\": ChatOpenAI(model=\"gpt-4o-mini\"),\n",
    "    \"smart\": ChatOpenAI(model=\"gpt-4o\"),\n",
    "}\n",
    "\n",
    "@wrap_model_call\n",
    "def select_model(request, state):\n",
    "    \"\"\"복잡도에 따라 모델 동적 선택\"\"\"\n",
    "    messages = state.get(\"messages\", [])\n",
    "\n",
    "    # 메시지가 길거나 복잡하면 고성능 모델 사용\n",
    "    if len(messages) > 10 or any(\"분석\" in str(m) for m in messages):\n",
    "        request.model = models[\"smart\"]\n",
    "    else:\n",
    "        request.model = models[\"fast\"]\n",
    "\n",
    "    return request\n",
    "\n",
    "agent = create_agent(\n",
    "    model=models[\"fast\"],  # 기본 모델\n",
    "    tools=[search_tool],\n",
    "    middleware=[select_model]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd72b609",
   "metadata": {},
   "source": [
    "### Tools (도구)\n",
    "에이전트가 실제 작업을 수행할 수 있게 해주는 기능입니다. \n",
    "도구를 통해 에이전트는 다음과 같은 작업을 수행할 수 있습니다.\n",
    "\n",
    "* 웹 검색으로 실시간 정보 조회\n",
    "* 코드 실행 및 계산\n",
    "* 데이터베이스 쿼리\n",
    "* 외부 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee347131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d3/h50bjcxd1nxb6d98lbh8jhgw0000gn/T/ipykernel_81535/1239173190.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(max_results=3)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TavilySearchResults\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 내장 도구\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m search_tool \u001b[38;5;241m=\u001b[39m \u001b[43mTavilySearchResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 커스텀 도구\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate\u001b[39m(expression: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/study/langchain/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:239\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     emit_warning()\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/langchain/venv/lib/python3.11/site-packages/langchain_community/tools/tavily_search/tool.py:165\u001b[0m, in \u001b[0;36mTavilySearchResults.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    161\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_wrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TavilySearchAPIWrapper(\n\u001b[1;32m    162\u001b[0m         tavily_api_key\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/langchain/venv/lib/python3.11/site-packages/langchain_core/tools/base.py:541\u001b[0m, in \u001b[0;36mBaseTool.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema must be a subclass of pydantic BaseModel or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma JSON schema dict. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m     )\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/study/langchain/venv/lib/python3.11/site-packages/langchain_core/load/serializable.py:117\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/study/langchain/venv/lib/python3.11/site-packages/pydantic/main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    257\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "# 내장 도구\n",
    "search_tool = TavilySearchResults(max_results=3)\n",
    "\n",
    "# 커스텀 도구\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"수학 표현식을 계산합니다.\"\"\"\n",
    "    return str(eval(expression))\n",
    "\n",
    "# 여러 도구를 에이전트에 등록\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool, calculate],\n",
    "    system_prompt=\"당신은 검색과 계산을 수행하는 어시스턴트입니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2a7f7",
   "metadata": {},
   "source": [
    "에이전트는 도구를 다음과 같이 활용합니다.\n",
    "\n",
    "* 여러 도구를 순차적으로 호출 (이전 결과를 다음 도구 입력으로 사용)\n",
    "* 독립적인 도구를 병렬로 호출 (효율성 향상)\n",
    "* 도구 실행 실패 시 재시도 또는 대체 도구 사용\n",
    "* 도구 실행 결과를 상태에 저장하여 이후 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90346ebc",
   "metadata": {},
   "source": [
    "### System Prompt\n",
    "에이전트의 역할과 동작 방식을 정의하는 지침입니다.\n",
    "\n",
    "* 효율적인 시스템 프롬프트 작성\n",
    "    역할 정의: 에이전트가 수행할 역할을 명확히 기술\n",
    "    작업 방식: 도구 사용 순서와 방법을 안내\n",
    "    제약 조건: 하지 말아야 할 것들을 명시\n",
    "    출력 형식: 답변 형식을 지정 (마크다운, JSON 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"당신은 금융 분석 전문 어시스턴트입니다.\n",
    "\n",
    "## 역할\n",
    "- 주식, 채권, 암호화폐 등 금융 정보를 분석합니다.\n",
    "- 최신 시장 동향을 검색하여 정확한 정보를 제공합니다.\n",
    "\n",
    "## 작업 방식\n",
    "1. 사용자 질문을 분석합니다.\n",
    "2. 필요한 정보를 웹 검색으로 조회합니다.\n",
    "3. 수치 계산이 필요하면 계산 도구를 사용합니다.\n",
    "4. 결과를 종합하여 명확하게 답변합니다.\n",
    "\n",
    "## 주의사항\n",
    "- 투자 조언은 제공하지 않습니다.\n",
    "- 출처를 명시합니다.\n",
    "\"\"\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool, calculate],\n",
    "    system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ad2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 시스템 프롬프트\n",
    "# 런타임에 상태나 사용자 정보에 따라 프롬프트를 동적으로 생성해야 할 때는 @dynamic_prompt 미들웨어를 사용합니다\n",
    "from langchain.agents.middleware import dynamic_prompt\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "@dynamic_prompt\n",
    "def personalized_prompt(request, state):\n",
    "    \"\"\"사용자 정보에 따라 프롬프트 동적 생성\"\"\"\n",
    "    user_name = state.get(\"user_name\", \"사용자\")\n",
    "    language = state.get(\"language\", \"ko\")\n",
    "\n",
    "    if language == \"ko\":\n",
    "        return f\"\"\"안녕하세요 {user_name}님, 저는 AI 어시스턴트입니다.\n",
    "\n",
    "## 역할\n",
    "- 사용자의 질문에 친절하게 답변합니다.\n",
    "- 필요시 도구를 사용하여 정보를 조회합니다.\n",
    "\n",
    "## 주의사항\n",
    "- 항상 한국어로 답변합니다.\n",
    "- 존댓말을 사용합니다.\"\"\"\n",
    "    else:\n",
    "        return f\"Hello {user_name}, I'm your AI assistant.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[search_tool],\n",
    "    middleware=[personalized_prompt]  # 동적 프롬프트 미들웨어\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca735791",
   "metadata": {},
   "source": [
    "- 대화 히스토리 관리\n",
    "여러 턴의 대화를 처리하려면 메시지 히스토리를 유지합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# 대화 히스토리 초기화\n",
    "messages = []\n",
    "\n",
    "# 첫 번째 대화\n",
    "messages.append(HumanMessage(content=\"서울 날씨 알려줘\"))\n",
    "result = agent.invoke({\"messages\": messages})\n",
    "\n",
    "# 응답을 히스토리에 추가\n",
    "messages.extend(result[\"messages\"])\n",
    "\n",
    "# 후속 질문 (컨텍스트 유지)\n",
    "messages.append(HumanMessage(content=\"그럼 우산 필요할까?\"))\n",
    "result = agent.invoke({\"messages\": messages})\n",
    "\n",
    "print(result[\"messages\"][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d3036",
   "metadata": {},
   "source": [
    "## 내장 도구\n",
    "도구(Tools)는 모델이 호출하도록 설계된 유틸리티입니다. \n",
    "도구의 입력은 모델이 생성하도록 설계되었고, 출력은 모델에 다시 전달되도록 설계되었습니다. \n",
    "툴킷(Toolkit)은 함께 사용되도록 의도된 도구들의 모음입00ㅔ니다.\n",
    "\n",
    "LangChain은 다양한 작업을 수행하기 위한 여러 내장 도구를 제공합니다. \n",
    "이러한 도구들은 언어 모델의 기능을 확장하고 외부 데이터 소스와의 상호작용을 가능하게 합니다.\n",
    "https://wikidocs.net/261567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c7c98",
   "metadata": {},
   "source": [
    "## 커스텀 도구\n",
    "LangChain v1.0에서는 @tool 데코레이터를 사용하여 커스텀 도구를 쉽게 만들고, create_agent()를 통해 에이전트와 통합할 수 있습니다\n",
    "https://wikidocs.net/261571"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d62f10",
   "metadata": {},
   "source": [
    "### 에러 처리미들 웨어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806345f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 에러 처리 미들웨어\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "import requests\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"도구 실행 중 발생하는 에러를 처리하는 미들웨어\"\"\"\n",
    "    try:\n",
    "        # 원래 도구 실행\n",
    "        return handler(request)\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        # HTTP 에러 처리\n",
    "        return ToolMessage(\n",
    "            content=f\"날씨 API 요청 실패: {e.response.status_code}\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # 네트워크 에러 처리\n",
    "        return ToolMessage(\n",
    "            content=f\"네트워크 오류 발생: {str(e)}\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # 기타 에러 처리\n",
    "        return ToolMessage(\n",
    "            content=f\"도구 실행 오류: {str(e)}\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "# 미들웨어가 적용된 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    middleware=[handle_tool_errors],\n",
    "    system_prompt=\"당신은 날씨 정보 제공 전문 어시스턴트입니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재시도 미들웨어\n",
    "# 실패 시 자동으로 재시도하는 미들웨어 추가\n",
    "import time\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "@wrap_tool_call\n",
    "def retry_on_failure(request, handler, max_retries=3, delay=1):\n",
    "    \"\"\"실패 시 재시도하는 미들웨어\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return handler(request)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                # 마지막 시도 실패 시 에러 메시지 반환\n",
    "                return ToolMessage(\n",
    "                    content=f\"{max_retries}번 재시도 후 실패: {str(e)}\",\n",
    "                    tool_call_id=request.tool_call[\"id\"]\n",
    "                )\n",
    "            # 재시도 전 대기\n",
    "            time.sleep(delay * (attempt + 1))\n",
    "\n",
    "# 여러 미들웨어 적용\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_weather],\n",
    "    middleware=[retry_on_failure, handle_tool_errors],\n",
    "    system_prompt=\"당신은 날씨 정보 제공 전문 어시스턴트입니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e44defe",
   "metadata": {},
   "source": [
    "미들웨어는 리스트 순서대로 실행됩니다. \n",
    "위 예제에서는\n",
    "1. retry_on_failure가 먼저 실행되어 재시도 로직 처리 \n",
    "2. handle_tool_errors가 두 번째로 실행되어 세부 에러 처리\n",
    "\n",
    "따라서 재시도가 먼저, 에러 처리가 나중에 오는 것이 일반적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa5408",
   "metadata": {},
   "source": [
    "## ToolRuntime & 컨택스트\n",
    "ToolRuntime은 도구 함수에 자동으로 주입되는 특별한 파라미터입니다. \n",
    "LLM에게는 노출되지 않으면서 도구 내부에서 다음 정보에 접근할 수 있게 해줍니다.\n",
    "\n",
    "|속성|설명|\n",
    "|--|--|\n",
    "|state|에이전트 상태 (대화 히스토리, 커스텀 필드 등)|\n",
    "|context|불변 컨텍스트 (사용자 ID, 세션 정보 등)|\n",
    "|store|장기 메모리 저장소|\n",
    "|stream_writer|실시간 스트리밍 출력|\n",
    "|config|RunnableConfig 설정|\n",
    "|tool_call_id|현재 도구 호출 ID|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 사용법\n",
    "# 도구 함수에 runtime: ToolRuntime 파라미터를 추가하면 자동으로 주입됩니다.\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def get_user_preference(category: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사용자의 선호도를 조회합니다.\n",
    "\n",
    "    Args:\n",
    "        category: 조회할 카테고리 (food, music, etc.)\n",
    "    \"\"\"\n",
    "    # runtime에서 사용자 ID 가져오기\n",
    "    user_id = runtime.context.get(\"user_id\", \"anonymous\")\n",
    "\n",
    "    # store에서 사용자 선호도 조회\n",
    "    preferences = runtime.store.get(user_id, {})\n",
    "\n",
    "    return f\"{user_id}님의 {category} 선호도: {preferences.get(category, '정보 없음')}\"\n",
    "\n",
    "# runtime: ToolRuntime은 LLM에게 노출되지 않습니다\n",
    "# 함수 시그니처에 있어도 LLM은 이 파라미터를 생성하지 않습니다\n",
    "# 시스템이 자동으로 런타임 정보를 주입합니다\n",
    "# 기존의 InjectedState, InjectedStore, InjectedToolCallId를 대체\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28bd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State (상태) 접근\n",
    "# 상태 읽기\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "# 커스텀 상태 정의\n",
    "class MyAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    user_name: str\n",
    "    visit_count: int\n",
    "\n",
    "@tool\n",
    "def greet_user(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사용자에게 맞춤 인사를 합니다.\"\"\"\n",
    "    state = runtime.state\n",
    "\n",
    "    user_name = state.get(\"user_name\", \"손님\")\n",
    "    visit_count = state.get(\"visit_count\", 0)\n",
    "\n",
    "    if visit_count == 0:\n",
    "        return f\"안녕하세요, {user_name}님! 처음 오셨군요. 환영합니다!\"\n",
    "    else:\n",
    "        return f\"반갑습니다, {user_name}님! {visit_count}번째 방문이시네요.\"\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 커스텀 상태를 사용하는 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[greet_user],\n",
    "    state_schema=MyAgentState,\n",
    "    system_prompt=\"사용자 맞춤 서비스를 제공하는 어시스턴트입니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a05941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 업데이트\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langgraph.types import Command\n",
    "\n",
    "@tool\n",
    "def increment_counter(runtime: ToolRuntime) -> Command:\n",
    "    \"\"\"방문 카운터를 증가시킵니다.\"\"\"\n",
    "    current_count = runtime.state.get(\"visit_count\", 0)\n",
    "\n",
    "    # 상태 업데이트를 포함한 Command 반환\n",
    "    return Command(\n",
    "        update={\n",
    "            \"visit_count\": current_count + 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def save_preference(category: str, value: str, runtime: ToolRuntime) -> Command:\n",
    "    \"\"\"사용자 선호도를 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        category: 카테고리 (food, music, movie 등)\n",
    "        value: 선호 값\n",
    "    \"\"\"\n",
    "    current_prefs = runtime.state.get(\"preferences\", {})\n",
    "    current_prefs[category] = value\n",
    "\n",
    "    return Command(\n",
    "        update={\"preferences\": current_prefs},\n",
    "        # 선택적: 다음 단계로 이동\n",
    "        # goto=\"next_node\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64265a82",
   "metadata": {},
   "source": [
    "### Context (컨텍스트) 접근\n",
    "컨텍스트는 에이전트 실행 시 전달되는 불변(immutable) 정보입니다. \n",
    "사용자 ID, 세션 정보, API 키 등을 안전하게 전달할 수 있습니다.\n",
    "\n",
    "LangChain 1.0에서는 context 파라미터를 직접 전달하는 새로운 방식을 제공합니다. \n",
    "기존 config[\"configurable\"] 방식보다 타입 안전하고 명확합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6ccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='내 정보 알려줘', additional_kwargs={}, response_metadata={}, id='d1b9a550-62f7-49f9-9385-88b31bb8d1e4'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 53, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'id': 'chatcmpl-D2ufXiXGm57rWw7o1x7YU3fnsFmHt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c0398-aa2c-7ac0-b270-7b2c6cace263-0', tool_calls=[{'name': 'get_user_info', 'args': {}, 'id': 'call_rzRvn7d5RXADCSDbjbVy09vH', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 53, 'output_tokens': 11, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='User: user_123, Region: KR', name='get_user_info', id='fcda072f-09b8-4f72-a054-38c3e79c264c', tool_call_id='call_rzRvn7d5RXADCSDbjbVy09vH'), AIMessage(content='당신의 사용자 정보는 다음과 같습니다:\\n- 사용자 ID: user_123\\n- 지역: 대한민국(KR)', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 82, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_deacdd5f6f', 'id': 'chatcmpl-D2ufYGKX1GgykcW10t9W88eqVDFeq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c0398-b16b-7d01-a1e3-3ac0efa9dce6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 82, 'output_tokens': 26, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'당신의 사용자 정보는 다음과 같습니다:\\n- 사용자 ID: user_123\\n- 지역: 대한민국(KR)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# Context 타입 정의 (dataclass 권장)\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    api_key: str\n",
    "    region: str = \"KR\"\n",
    "\n",
    "@tool\n",
    "def get_user_info(runtime: ToolRuntime[AppContext]) -> str:\n",
    "    \"\"\"사용자 정보를 조회합니다.\"\"\"\n",
    "    # 타입 안전한 컨텍스트 접근\n",
    "    ctx = runtime.context\n",
    "    return f\"User: {ctx.user_id}, Region: {ctx.region}\"\n",
    "\n",
    "# 에이전트 생성 시 context_schema 지정\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[get_user_info],\n",
    "    context_schema=AppContext,  # 컨텍스트 스키마 지정\n",
    "    system_prompt=\"사용자 맞춤 서비스 어시스턴트입니다.\"\n",
    ")\n",
    "\n",
    "# 실행 시 context 파라미터로 직접 전달\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"내 정보 알려줘\"}]},\n",
    "    context=AppContext(\n",
    "        user_id=\"user_123\",\n",
    "        session_id=\"sess_abc\",\n",
    "        api_key=\"sk-xxx\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(result)\n",
    "result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c212f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인증 정보 전달\n",
    "import httpx\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "async def fetch_user_data(endpoint: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사용자 데이터를 외부 API에서 조회합니다.\n",
    "\n",
    "    Args:\n",
    "        endpoint: API 엔드포인트 경로\n",
    "    \"\"\"\n",
    "    # 컨텍스트에서 인증 정보 가져오기\n",
    "    api_key = runtime.context.get(\"api_key\")\n",
    "    base_url = runtime.context.get(\"api_base_url\", \"https://api.example.com\")\n",
    "\n",
    "    if not api_key:\n",
    "        return \"API 키가 설정되지 않았습니다.\"\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        response = await client.get(\n",
    "            f\"{base_url}/{endpoint}\",\n",
    "            headers={\"Authorization\": f\"Bearer {api_key}\"}\n",
    "        )\n",
    "        return response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "161c205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seohyuktaek/Documents/study/langchain/venv/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='context', input_value={'user_id': '서혁택'}, input_type=dict])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seohyuktaek/Documents/study/langchain/venv/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `none` - serialized value may not be as expected [field_name='context', input_value={'user_id': '서혁택'}, input_type=dict])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'안녕하세요, 양지웅님! 이제 당신의 이름을 기억했습니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store (장기메모리) 접근\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "@tool\n",
    "def remember_fact(key: str, value: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사실을 장기 메모리에 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        key: 저장할 키\n",
    "        value: 저장할 값\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    user_id = runtime.context.get(\"user_id\", \"default\")\n",
    "\n",
    "    # Store에 저장\n",
    "    namespace = (\"user_facts\", user_id)\n",
    "    runtime.store.put(namespace, key, {\"value\": value})\n",
    "\n",
    "    return f\"'{key}: {value}'를 기억했습니다.\"\n",
    "\n",
    "@tool\n",
    "def recall_fact(key: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"장기 메모리에서 사실을 조회합니다.\n",
    "    \n",
    "    Args:\n",
    "        key: 조회할 키\n",
    "    \"\"\"\n",
    "    print(2)\n",
    "    user_id = runtime.context.get(\"user_id\", \"default\")\n",
    "    # Store에서 조회\n",
    "    namespace = (\"user_facts\", user_id)\n",
    "    item = runtime.store.get(namespace, key)\n",
    "\n",
    "    if item:\n",
    "        return f\"'{key}'에 대한 기억: {item.value['value']}\"\n",
    "    else:\n",
    "        return f\"'{key}'에 대한 기억이 없습니다.\"\n",
    "\n",
    "# Store 설정\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Store가 연결된 에이전트 생성\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[remember_fact, recall_fact],\n",
    "    store=store,\n",
    "    system_prompt=\"사용자의 정보를 기억하는 어시스턴트입니다.\"\n",
    ")\n",
    "\n",
    "# result = agent.invoke({\n",
    "#     \"messages\": [{\"content\": \"사용자 이름은 누구?\", \"role\": \"user\"}],\n",
    "# },\n",
    "#     context= {\n",
    "#         \"user_id\": \"서혁택\"\n",
    "#     })\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"content\": \"나는 양지웅이야 나를 기억하겠어?\", \"role\": \"user\"}],\n",
    "},\n",
    "    context= {\n",
    "        \"user_id\": \"서혁택\"\n",
    "    })\n",
    "\n",
    "result[\"messages\"][-1].content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store 활용 패턴\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def update_user_profile(field: str, value: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사용자 프로필을 업데이트합니다.\n",
    "\n",
    "    Args:\n",
    "        field: 업데이트할 필드 (name, email, preference 등)\n",
    "        value: 새 값\n",
    "    \"\"\"\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    namespace = (\"profiles\", user_id)\n",
    "\n",
    "    # 기존 프로필 조회\n",
    "    existing = runtime.store.get(namespace, \"profile\")\n",
    "    profile = existing.value if existing else {}\n",
    "\n",
    "    # 필드 업데이트\n",
    "    profile[field] = value\n",
    "\n",
    "    # 저장\n",
    "    runtime.store.put(namespace, \"profile\", profile)\n",
    "\n",
    "    return f\"프로필 업데이트 완료: {field} = {value}\"\n",
    "\n",
    "@tool\n",
    "def get_user_profile(runtime: ToolRuntime) -> str:\n",
    "    \"\"\"사용자 프로필 전체를 조회합니다.\"\"\"\n",
    "    user_id = runtime.context.get(\"user_id\")\n",
    "    namespace = (\"profiles\", user_id)\n",
    "\n",
    "    existing = runtime.store.get(namespace, \"profile\")\n",
    "\n",
    "    if existing:\n",
    "        profile = existing.value\n",
    "        lines = [f\"- {k}: {v}\" for k, v in profile.items()]\n",
    "        return \"사용자 프로필:\\n\" + \"\\n\".join(lines)\n",
    "    else:\n",
    "        return \"저장된 프로필이 없습니다.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cb4757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream Writer ( 실시간 스트리밍)\n",
    "# stream_writer를 사용하면 도구 실행 중에 실시간으로 진행 상황을 스트리밍할 수 있습니다.\n",
    "import time\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "@tool\n",
    "def long_running_task(steps: int, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"여러 단계의 작업을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        steps: 수행할 단계 수\n",
    "    \"\"\"\n",
    "    writer = runtime.stream_writer\n",
    "\n",
    "    for i in range(1, steps + 1):\n",
    "        # 진행 상황 스트리밍\n",
    "        writer({\"progress\": f\"단계 {i}/{steps} 처리 중...\"})\n",
    "        time.sleep(0.5)  # 작업 시뮬레이션\n",
    "\n",
    "    writer({\"status\": \"완료\"})\n",
    "    return f\"총 {steps}단계 작업이 완료되었습니다.\"\n",
    "\n",
    "@tool\n",
    "def fetch_multiple_sources(sources: list[str], runtime: ToolRuntime) -> str:\n",
    "    \"\"\"여러 소스에서 데이터를 수집합니다.\n",
    "\n",
    "    Args:\n",
    "        sources: 조회할 소스 목록\n",
    "    \"\"\"\n",
    "    writer = runtime.stream_writer\n",
    "    results = []\n",
    "\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        # 현재 진행 상황 스트리밍\n",
    "        writer({\n",
    "            \"type\": \"progress\",\n",
    "            \"message\": f\"[{i}/{len(sources)}] {source} 조회 중...\"\n",
    "        })\n",
    "\n",
    "        # 실제 데이터 수집 로직\n",
    "        data = f\"{source}에서 수집한 데이터\"\n",
    "        results.append(data)\n",
    "\n",
    "        # 개별 결과 스트리밍\n",
    "        writer({\n",
    "            \"type\": \"partial_result\",\n",
    "            \"source\": source,\n",
    "            \"data\": data\n",
    "        })\n",
    "\n",
    "    return \"\\n\".join(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5808ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'progress': '단계 1/5 처리 중...'}\n",
      "{'progress': '단계 2/5 처리 중...'}\n",
      "{'progress': '단계 3/5 처리 중...'}\n",
      "{'progress': '단계 4/5 처리 중...'}\n",
      "{'progress': '단계 5/5 처리 중...'}\n",
      "{'status': '완료'}\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[long_running_task, fetch_multiple_sources],\n",
    "    system_prompt=\"데이터 수집 어시스턴트입니다.\"\n",
    ")\n",
    "\n",
    "# stream_mode=\"custom\"으로 실행해야 stream_writer 출력을 받을 수 있음\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"5단계 작업 실행해줘\"}]},\n",
    "    stream_mode=\"custom\"\n",
    "):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a17f07ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'어떤 작업을 추적할지를 명시해주시면, 해당 작업을 실행하여 추적할 수 있습니다. 수행할 작업의 이름이나 설명을 알려주세요.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ToolCallID\n",
    "# tool_call_id는 현재 도구 호출의 고유 식별자입니다. 디버깅이나 로깅에 유용합니다.\n",
    "import logging\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@tool\n",
    "def tracked_operation(action: str, runtime: ToolRuntime) -> str:\n",
    "    \"\"\"추적 가능한 작업을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        action: 수행할 작업\n",
    "    \"\"\"\n",
    "    tool_call_id = runtime.tool_call_id\n",
    "\n",
    "    # 로깅에 tool_call_id 포함\n",
    "    logger.info(f\"[{tool_call_id}] 작업 시작: {action}\")\n",
    "\n",
    "    # 작업 수행\n",
    "    result = f\"'{action}' 작업 완료\"\n",
    "\n",
    "    logger.info(f\"[{tool_call_id}] 작업 완료\")\n",
    "\n",
    "    return result\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[tracked_operation],\n",
    "    system_prompt=\"추적 가능한 작업을 수행하는 어시스턴트입니다.\"\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"추적 가능한 작업 실행해줘\"}]\n",
    "})\n",
    "\n",
    "result[\"messages\"][-1].content\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
