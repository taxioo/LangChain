{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b819637",
   "metadata": {},
   "source": [
    "# RAG 기법\n",
    "RAG(Retrieval-Augmented Generation) 기법은 기존의 대규모 언어 모델(LLM)을 확장하여, 주어진 컨텍스트나 질문에 대해 더욱 정확하고 풍부한 정보를 제공하는 방법입니다. \n",
    "모델이 학습 데이터에 포함되지 않은 외부 데이터를 실시간으로 검색(retrieval)하고, 이를 바탕으로 답변을 생성(generation)하는 과정을 포함합니다. 특히 환각(생성된 내용이 사실이 아닌 것으로 오인되는 현상)을 방지하고, 모델이 최신 정보를 반영하거나 더 넓은 지식을 활용할 수 있게 합니다.\n",
    "\n",
    "1. RAG 모델의 기본 구조\n",
    "    - 검색 단계(Retrieval Phase)\n",
    "     - 사용자의 질문이나 컨텍스트를 입력으로 받아서, 이와 관련된 외부 데이터를 검색하는 단계입니다. 이 때 검색 엔진이나 데이터베이스 등 다양한 소스에서 필요한 정보를 찾아냅니다. 검색된 데이터는 질문에 대한 답변을 생성하는데 적합하고 상세한 정보를 포함하는 것을 목표로 합니다.\n",
    "\n",
    "    - 생성 단계(Generation Phase)\n",
    "      - 검색된 데이터를 기반으로 LLM 모델이 사용자의 질문에 답변을 생성하는 단계입니다. 이 단계에서 모델은 검색된 정보와 기존의 지식을 결합하여, 주어진 질문에 대한 답변을 생성합니다.\n",
    "2. RAG 모델의 장점\n",
    "    - 풍부한 정보 제공\n",
    "      - RAG 모델은 검색을 통해 얻은 외부 데이터를 활용하여, 보다 구체적이고 풍부한 정보를 제공할 수 있습니다.\n",
    "    - 실시간 정보 반영\n",
    "      - 최신 데이터를 검색하여 반영함으로써, 모델이 실시간으로 변화하는 정보에 대응할 수 있습니다.\n",
    "    - 환각 방지\n",
    "      - 검색을 통해 실제 데이터에 기반한 답변을 생성함으로써, 환각 현상이 발생할 위험을 줄이고 정확도를 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9de9ec",
   "metadata": {},
   "source": [
    "## RAG 개요\n",
    "RAG(Retrieval-Augmented Generation) 파이프라인은 기존의 언어 모델에 검색 기능을 추가하여, 주어진 질문이나 문제에 대해 더 정확하고 풍부한 정보를 기반으로 답변을 생성할 수 있게 해줍니다. \n",
    "이 파이프라인은 크게 `데이터 로드`, `텍스트 분할`, `인덱싱`, `검색`, `생성`의 다섯 단계로 구성됩니다\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5517378b",
   "metadata": {},
   "source": [
    "1. 데이터 로드(Load Data)\n",
    "   RAG에 사용할 데이터를 불러오는 단계입니다. \n",
    "   외부 데이터 소스에서 정보를 수집하고, 필요한 형식으로 변환하여 시스템에 로드합니다. \n",
    "   \n",
    "   예를 들면 공개 데이터셋, 웹 크롤링을 통해 얻은 데이터, 또는 사전에 정리된 자료일 수 있습니다. 가져온 데이터는 검색에 사용될 지식이나 정보를 담고 있어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ea60a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "13307\n",
      "체적으로 어기고 있다면 규범 준수를 위해 좀 더 빠르게 강력한 수단을 이용해야 합니다. 특히 정책 문서에 명시된 원칙을 지키지 않는 것은 대부분의 경우 다른 사용자에게 받아들여지지 않습니다 (다른 분들에게 예외 상황임을 설득할 수 있다면 가능하기는 하지만요). 이는 당신을 포함해서 편집자 개개인이 정책과 지침을 직접 집행 및 적용한다는 것을 의미합니다.\n",
      "특정 사용자가 명백히 정책에 반하는 행동을 하거나 정책과 상충되는 방식으로 지침을 어기는 경우, 특히 의도적이고 지속적으로 그런 행위를 하는 경우 해당 사용자는 관리자의 제재 조치로 일시적, 혹은 영구적으로 편집이 차단될 수 있습니다. 영어판을 비롯한 타 언어판에서는 일반적인 분쟁 해결 절차로 끝낼 수 없는 사안은 중재위원회가 개입하기도 합니다.\n",
      "문서 내용\n",
      "정책과 지침의 문서 내용은 처음 읽는 사용자라도 원칙과 규범을 잘 이해할 수 있도록 다음 원칙을 지켜야 합니다.\n",
      "명확하게 작성하세요. 소수만 알아듣거나 준법률적인 단어, 혹은 지나치게 단순한 표현은 피해야 합니다. 명확하고, 직접적이고, 모호하지 않고, 구체적으로 작성하세요. 지나치게 상투적인 표현이나 일반론은 피하세요. 지침, 도움말 문서 및 기타 정보문 문서에서도 \"해야 합니다\" 혹은 \"하지 말아야 합니다\" 같이 직접적인 표현을 굳이 꺼릴 필요는 없습니다.\n",
      "가능한 간결하게, 너무 단순하지는 않게. 정책이 중언부언하면 오해를 부릅니다. 불필요한 말은 생략하세요. 직접적이고 간결한 설명이 마구잡이식 예시 나열보다 더 이해하기 쉽습니다. 각주나 관련 문서 링크를 이용하여 더 상세히 설명할 수도 있습니다.\n",
      "규칙을 만든 의도를 강조하세요. 사용자들이 상식대로 행동하리라 기대하세요. 정책의 의도가 명료하다면, 추가 설명은 필요 없죠. 즉 규칙을 '어떻게' 지키는지와 더불어 '왜' 지켜야 하는지 확실하게 밝혀야 합니다.\n",
      "범위는 분명히, 중복은 피하기. 되도록 앞부분에서 정책 및 지침의 목적과 범위를 분명하게 밝혀야 합니다. 독자 대부분은 도입부 초반만 읽고 나가버리니\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "# Data Loader - 웹페이지 데이터 가져오기\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 위키피디아 정책과 지침\n",
    "url = 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8'\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "# 웹페이지 텍스트 -> Documents\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "print(len(docs[0].page_content))\n",
    "print(docs[0].page_content[5000:6000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34fa00",
   "metadata": {},
   "source": [
    "2. 텍스트 분할\n",
    "   불러온 데이터를 작은 크기의 단위(chunk)로 분할하는 과정입니다. \n",
    "   `자연어 처리(NLP) 기술을 활용하여 큰 문서를 처리가 쉽도록 문단, 문장 또는 구 단위로 나누는 작업입니다. `\n",
    "   검색 효율성을 높이기 위한 중요한 과정입니다.\n",
    "\n",
    "   다음 코드는 `RecursiveCharacterTextSplitter`라는 텍스트 분할 도구를 사용하고 있습니다. (이 도구에 대해서는 Text Splitter 챕터에서 상세하게 다룰 예정입니다. ) 간략하게 설명하면 12552 개의 문자로 이루어진 긴 문장을 최대 1000글자 단위로 분할하는 것입니다. 200글자는 각 분할마다 겹치게 하여 문맥이 잘려나가지 않고 유지되게 합니다. 실행 결과를 보면 18개 조각으로 나눠지게 됩니다.\n",
    "\n",
    "   `LLM 모델이나 API의 입력 크기에 대한 제한`이 있기 때문에, 제한에 걸리지 않도록 적정한 크기로 텍스트의 길이를 줄일 필요가 있습니다. 그리고, 프롬프트가 지나치게 길어질 경우 중요한 정보가 상대적으로 희석되는 문제가 있을 수도 있습니다. \n",
    "   따라서, 적정한 크기로 텍스트를 분할하는 과정이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94da747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "page_content='정책과 지침 관련 총의는 추세가 뚜렷해야 하지만, 만장일치일 필요는 없습니다.\n",
      "제안 작성자 이외에도 공동체 전체에 노출되어 있어야 합니다.\n",
      "제안 문서의 영향력을 고려하세요.\n",
      "토론 중 중대한 문제가 제기되었는가?\n",
      "다른 정책 또는 지침과 모순되지는 않는가?\n",
      "새로 제정된 다른 정책 또는 지침과 병합될 수도 있는가?\n",
      "이미 존재하는 정책 및 지침과 중복되지는 않는가?\n",
      "제안 처리는 단순 득표수로 결정되지 않습니다. 투표는 토론을 대체하지 않으며, 득표수가 총의와 합치하는 것도 아닙니다.\n",
      "총의가 불분명하고, 이 상황이 개선될 것 같지 않다면 이는 거부됩니다.\n",
      "채택 토론은 세 가지로 귀결됩니다. 채택, 총의 없음, 거부.  토론 종결 이후 {{제안}} 틀을 제거하고 알맞은 틀을 삽입하세요. 가령 {{정책}}, {{지침}}, {{수필}}, {{거부}}  등을 말이죠. 그 외에 다양한 틀에는 위키백과 이름공간 관련 틀 (영어 위키백과)을 참고하세요.\n",
      "제안이 통과되었다면, 위키백과:사랑방에 통과된 정책의 주요 내용을 정리해서 새 제안이 정책/지침화되었음을 공지해야 합니다. 제안이 거부되었다면 특별한 합의가 있지 않는 이상 {{거부}} 틀을 지우지 않습니다. 이후 문서 개선 등을 통해 새 규칙으로 발전할 수도 있고, 다른 정책 개정 시 논란이 없는 부분에 한해 도입되기도 되기도 하니까요.\n",
      "내용 변경\n",
      "정책과 지침은 다른 위키백과 문서처럼 편집할 수 있습니다. 사전에 변경 사항을 논의하거나 문자화된 총의 기록를 구비할 필요는 없습니다. 그러나 정책과 지침은 민감하고 복잡합니다. 사용자들은 모든 편집 시 공동체의 관점을 충실히 반영하도록, 실수로 새로운 오류 또는 혼동의 원인이 발생하지 않도록 주의해야 합니다.' metadata={'source': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8', 'title': '위키백과:정책과 지침 - 위키백과, 우리 모두의 백과사전', 'language': 'ko'}\n"
     ]
    }
   ],
   "source": [
    "# Text Split (Documents -> small chunks: Documents)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(splits))\n",
    "print(splits[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c384c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\'제안\\'은 완전 새로운 원칙이라기보다, 기존의 불문율이나 토론 총의의 문서를 통한 구체화에 가깝습니다. 많은 사람들이 쉽게 제안을 받아들이도록 하기 위해서는, 기초적인 원칙을 우선 정하고 기본 틀을 짜야 합니다. 정책과 지침의 기본 원칙은 \"왜 지켜야 하는가?\", \"어떻게 지켜야 하는가?\" 두 가지입니다. 특정 원칙을 정책이나 지침으로 확립하기 위해서는 우선 저 두 가지 물음에 성실하게 답하는 제안 문서를 작성해야 합니다.\\n좋은 아이디어를 싣기 위해 사랑방이나 관련 위키프로젝트에 도움을 구해 피드백을 요청할 수 있습니다. 이 과정에서 공동체가 어느 정도 받아들일 수 있는 원칙이 구체화됩니다. 많은 이와의 토론을 통해 공감대가 형성되고 제안을 개선할 수 있습니다.\\n정책이나 지침은 위키백과 내의 모든 편집자들에게 적용되는 원칙이므로 높은 수준의 총의가 요구됩니다. 제안 문서가 잘 짜여졌고 충분히 논의되었다면, 더 많은 공동체의 편집자와 논의를 하기 위해 승격 제안을 올려야 합니다. 제안 문서 맨 위에 {{제안}}을 붙여 제안 안건임을 알려주고, 토론 문서에 {{의견 요청}}을 붙인 뒤 채택 제안에 관한 토론 문단을 새로 만들면 됩니다. 많은 편집자들에게 알리기 위해 관련 내용을 {{위키백과 소식}}에 올리고 사랑방에 이를 공지해야 하며, 합의가 있을 경우 미디어위키의 sitenotice(위키백과 최상단에 노출되는 구역)에 공지할 수도 있습니다. \\n차후 공지가 불충분했다는 이의 제기를 피하려면, 위의 링크를 이용하여 공지하세요. 공지에 비중립적인 단어를 사용하는 등의 선전 행위는 피하세요.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page_content 속성\n",
    "splits[10].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44edc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:%EC%A0%95%EC%B1%85%EA%B3%BC_%EC%A7%80%EC%B9%A8',\n",
       " 'title': '위키백과:정책과 지침 - 위키백과, 우리 모두의 백과사전',\n",
       " 'language': 'ko'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# metadata 속성\n",
    "splits[10].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417dce4",
   "metadata": {},
   "source": [
    "3. 인덱싱\n",
    "    분할된 텍스트를 `검색 가능한 형태`로 만드는 단계입니다. \n",
    "    인덱싱은 `검색 시간을 단축`시키고, `검색의 정확도를 높이는 데 중요한 역할`을 합니다. \n",
    "    LangChain 라이브러리를 사용하여 `텍스트를 임베딩`으로 변환하고, 이를 `저장`한 후, 저장된 임베딩을 기반으로 유사성 검색을 수행하는 과정을 보여줍니다.\n",
    "\n",
    "    간략하게 설명하면 OpenAI의 임베딩 모델을 사용하여 텍스트를 벡터로 변환하고, 이를 Chroma 벡터저장소에 저장합니다. \n",
    "    `vectorstore.similarity_search` 메소드는 주어진 쿼리 문자열(\"격하 과정에 대해서 설명해주세요.\")에 대해 저장된 문서들 중에서 가장 유사한 문서들을 찾아냅니다. \n",
    "    이때 유사성은 `임베딩 간의 거리`(또는 유사도)로 계산됩니다. \n",
    "    4개의 문서가 반환되는데, 가장 유사도가 높은 첫 번째 문서를 출력하여 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358af019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "채택 과정\n",
      "한국어 위키백과에서 오랫동안 확립되어 온 정책과 지침의 대다수는, 영어 위키백과 설립 시 토대가 된 원칙에서 발전된 것들입니다. 물론 타 언어 위키백과의 원칙을 가져오는 것 말고도, 정책과 지침을 일반적인 문제와 문서 훼손 행위의 대응책으로 한국어 위키백과 내 공동체에서 자발적으로 세우기도 했습니다. 정책과 지침 대부분은 전례 없이 바로 받아들여지기보다, 공동체의 강력한 지지를 바탕으로 세워집니다. 정책과 지침의 제정 방법으로는 제안을 통한 수립, 기존의 수필 또는 지침의 정책화, 기존의 정책과 지침의 분할 또는 합병을 통한 재구성 등의 여러 방법이 있습니다.\n",
      "정책과 지침이 아닌 위키백과 내 운영과 관련된 문서에는 {{수필}}, {{정보문}}, {{위키백과 사용서}} 등을 붙여 구분해야 합니다.\n",
      "현재 정책이나 지침으로 제안된 문서는 분류:위키백과 제안에 모여 있습니다. 총의를 통해 채택이 거부된 제안은 분류:위키백과 거부된 제안을 참조하세요. 여러분들의 참여를 환영합니다.\n",
      "제안과 채택\n",
      " 백:아님 § 관료주의  문서를 참고하십시오. 단축백:제안\n",
      "제안 문서란 정책과 지침으로 채택하자고 의견을 묻는 문서이나 아직 위키백과 내에 받아들여지는 원칙으로 확립되지는 않은 문서입니다. {{제안}} 틀을 붙여 공동체 내에서 정책이나 지침으로 채택할 지 의견을 물을 수 있습니다. 제안 문서는 정책과 지침이 아니므로 아무리 실제 있는 정책이나 지침을 요약하거나 인용해서 다른 문서에 쓴다고 해도 함부로 정책이나 지침 틀을 붙여서는 안 됩니다.\n",
      "'제안'은 완전 새로운 원칙이라기보다, 기존의 불문율이나 토론 총의의 문서를 통한 구체화에 가깝습니다. 많은 사람들이 쉽게 제안을 받아들이도록 하기 위해서는, 기초적인 원칙을 우선 정하고 기본 틀을 짜야 합니다. 정책과 지침의 기본 원칙은 \"왜 지켜야 하는가?\", \"어떻게 지켜야 하는가?\" 두 가지입니다. 특정 원칙을 정책이나 지침으로 확립하기 위해서는 우선 저 두 가지 물음에 성실하게 답하는 제안 문서를 작성해야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# Indexing (Texts -> Embedding -> Store)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "docs = vectorstore.similarity_search(\"격하 과정에 대해서 설명해주세요.\")\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb025d26",
   "metadata": {},
   "source": [
    "4. 검색\n",
    "    사용자의 질문이나 주어진 컨텍스트에 가장 관련된 정보를 찾아내는 과정입니다. \n",
    "    사용자의 입력을 바탕으로 쿼리를 생성하고, 인덱싱된 데이터에서 가장 관련성 높은 정보를 검색합니다. \n",
    "    LangChain의 retriever 메소드를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40456031",
   "metadata": {},
   "source": [
    "5. 생성\n",
    "    검색된 정보를 바탕으로 사용자의 질문에 답변을 생성하는 최종 단계입니다. \n",
    "    LLM 모델에 검색 결과와 함께 사용자의 입력을 전달합니다. \n",
    "    모델은 사전 학습된 지식과 검색 결과를 결합하여 주어진 질문에 가장 적절한 답변을 생성합니다.\n",
    "\n",
    "    검색과 생성 단계를 수행하는 다음 코드를 살펴보겠습니다. \n",
    "    vectorstore.as_retriever() 메소드는 Chroma 벡터 스토어를 검색기로 사용하여 사용자의 질문과 관련된 문서를 검색합니다. \n",
    "    format_docs 함수는 검색된 문서들을 하나의 문자열로 반환합니다. \n",
    "    RAG 체인을 구성하고, 주어진 질문에 대한 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6edc7b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"제공된 문맥에는 '격하 과정'에 대한 설명이 포함되어 있지 않습니다. 따라서 격하 과정에 대한 정보를 제공할 수 없습니다.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "template = '''Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# Rretriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combine Documents\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Chain 연결\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain 실행\n",
    "rag_chain.invoke(\"격하 과정에 대해서 설명해주세요.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf534a6e",
   "metadata": {},
   "source": [
    "## RAG - Document Loader\n",
    "LangChain에서 `Document Loader는 다양한 소스에서 문서를 불러오고 처리하는 과정을 담당`합니다. \n",
    "특히 사전지식이 필요한 지식 기반의 태스크, 정보 검색, 데이터 처리 작업 등을 처리할 때 반드시 필요합니다. \n",
    "Document Loader의 주요 목적은 `효율적으로 문서 데이터를 수집하고, 사용 가능한 형식으로 변환`하는 것입니다.\n",
    "\n",
    "- 다양한 소스 지원\n",
    "  웹 페이지, PDF 파일, 데이터베이스 등 다양한 소스에서 문서를 불러올 수 있습니다.\n",
    "\n",
    "- 데이터 변환 및 정제\n",
    "  `불러온 문서 데이터를 분석하고 처리하여, 랭체인의 다른 모듈이나 알고리즘이 처리하기 쉬운 형태로 변환`합니다. \n",
    "  불필요한 데이터를 제거하거나, 구조를 변경할 수도 있습니다.\n",
    "\n",
    "- 효율적인 데이터 관리\n",
    "  대량의 문서 데이터를 효율적으로 관리하고, 필요할 때 쉽게 접근할 수 있도록 합니다. 이를 통해 검색 속도를 향상시키고, 전체 시스템의 성능을 높일 수 있습니다.\n",
    "\n",
    "사용 예시:\n",
    "웹 크롤링을 통해 특정 주제에 관한 기사나 논문을 자동으로 수집하고, 이를 분석하여 요약 정보를 생성하는 애플리케이션.\n",
    "기업 내부 문서 저장소에서 필요한 문서를 빠르게 검색하고, 관련 정보를 추출하여 보고서를 자동으로 작성하는 시스템"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8aaf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Replit is at the forefront of AI innovation with its platform that simplifies writing, running, and collaborating on code for over 30+ million developers. They recently released Replit Agent, which immediately went viral due to the incredible applications people could easily create with this tool.Behind the scenes, Replit Agent has a complex workflow which enables a highly custom agentic workflow with a high-degree of control and parallel execution. By using LangSmith, Replit gained deep visibility into their agent interactions to debug tricky issues. The level of complexity required for Replit Agent also pushed the boundaries of LangSmith. The LangChain and Replit teams worked closely together to add functionality to LangSmith that would satisfy their LLM observability needs. Specifically, there were three main areas that we innovated on:Improved performance and scale on large tracesAbility to search and filter within tracesThread view to enable human-in-the loop workflowsImproved performance and scale on large tracesMost other LLMOps solutions monitor individual API requests to LLM providers, offering a limited view of single LLM calls. In contrast, LangSmith from day one has focused on tracing the entire execution flow of an LLM application to provide a more holistic context. Tracing is important for agents due to their complex nature. It captures multiple LLM calls as well as other steps (retrieval, running code, etc). This gives you granular visibility into what’s happening, including at the inputs and outputs of each step, in order to understand the agent’s decision-making. Replit Agent was a ripe example for advanced tracing needs. Their agentic tool goes beyond simply reviewing and writing code, but also performs a wider range of functions – including planning, creating dev environments, installing dependencies, and deploying applications for users. As a result, Replit’s traces were very large - involving hundreds of steps. This posed significant challenges for ingesting data and displaying it in a visually meaningful way.To address this, the LangChain team improved their ingestion to efficiently process and store large volumes of trace data. They also improved LangSmith’s frontend rendering to display long-running agent traces seamlessly.Search and filter within traces to pinpoint issuesLangSmith has always supported search between traces, which allows users to find a single trace among hundreds of thousands based on events or full text search. But as Replit Agent’s traces got longer and longer, the Replit team needed to search within traces for specific events (oftentimes issues reported by alpha testers). This required augmenting existing search capabilities.In response, a new search pattern – searching within traces – was added to LangSmith. Instead of sifting and scrolling call-by-call within a large trace, users could now filter directly on a criteria they cared about (e.g. keywords in the inputs or outputs of a run). This greatly reduced Replit’s time needed to debug agent steps within a trace.Thread view to enable human-in-the-loop workflowsA key differentiator of Replit Agent was its emphasis on human-in-the-loop workflows. Replit Agent intends to be a tool where AI agents can collaborate effectively with human developers, who can come in and edit and correct agent trajectories as needed.With separate agents to perform roles like managing, editing, and verifying generated code,  Replit’s agents interacted with users continuously - often over long periods with multiple turns of conversation. However, monitoring these conversational flows was often difficult, as each user session would generate disjoint traces. To solve this, LangSmith’s thread view helped collate traces from multiple threads together that were related (i.e. from one conversation). This provided a logical view of all agent-user interactions across a multi-turn conversation, helping Replit better 1) find bottlenecks where users got stuck and 2) pinpoint areas where human intervention could be beneficial. ConclusionReplit is pushing the frontier of AI agent monitoring using LangSmith’s powerful observability features. By reducing the effort of loading long, heavy traces, the Replit team has greatly sped up the process of building and scaling complex agents. With faster debugging, improved trace visibility, and better handling of parallel tasks, Replit is setting the standard for AI-driven development.\t\n",
      "\n",
      "{'source': 'https://blog.langchain.dev/customers-replit/'}\n",
      "--------------------------------\n",
      "16\n",
      "page_content='stores the values that changed.Read-side optimizations:We're making use of a cursor for the list endpoint in order to efficiently fetch long thread histories when needed.Getting started on LangGraph v0.2Since LangGraph checkpointer libraries are implemented as namespace packages, you can import checkpointer interfaces and implementations the same way as before, using:from langgraph.checkpoint.base import BaseCheckpointSaverfrom langgraph.checkpoint.memory import MemorySaverfrom langgraph.checkpoint.sqlite import SqliteSaverfrom langgraph.checkpoint.postgres import PostgresSaverSince SQLite and Postgres checkpointers are provided via separate libraries, you will need to install them using pip install langgraph-checkpoint-sqlite or pip install langgraph-checkpoint-postgres.LangGraph checkpoint libraries will follow semantic versioning (starting with current release of 1.0), and any breaking changes in the main library will result in a major version bump for those libraries. For example,' metadata={'source': 'https://blog.langchain.dev/langgraph-v0-2/'}\n"
     ]
    }
   ],
   "source": [
    "# 웹 문서\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# 여러 개의 url 지정 가능\n",
    "url1 = \"https://blog.langchain.dev/customers-replit/\"\n",
    "url2 = \"https://blog.langchain.dev/langgraph-v0-2/\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url1, url2),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"article-header\", \"article-content\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)\n",
    "print(\"--------------------------------\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(len(splits))\n",
    "print(splits[10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2d180a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./내부분석환경_사용자매뉴얼(수정).docx']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디렉토리 폴더(Directory Loader)\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "files = glob(os.path.join('./', '*.docx'))\n",
    "files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "735bcf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'분석환경 생성 과정은 다음과 같습니다:\\n\\n### 1. 개인 분석환경 생성\\n1. **리소스 요청**: 분석과제에 소속된 사용자는 과제 관리자에게 할당받은 리소스를 기반으로 분석환경을 생성할 수 있습니다. 필요한 리소스를 요청하기 위해 [리소스 요청] 버튼을 클릭합니다.\\n2. **정보 입력**: 요청할 리소스의 정보를 입력합니다. 이때 CPU는 최소 2개, RAM은 최소 4GB 이상이어야 합니다.\\n3. **환경 추가**: [새 환경 추가+] 버튼을 클릭하여 개인 분석환경을 추가합니다.\\n4. **분석환경 추가**: 정보를 입력한 후 [분석환경 추가] 버튼을 클릭하여 환경을 생성합니다.\\n5. **확인**: 생성된 분석환경은 분석환경 페이지에서 확인할 수 있습니다. (분석환경 > 개인분석환경)\\n6. **사용 및 수정**: 생성된 분석환경은 자유롭게 사용할 수 있으며, 이후 수정도 가능합니다. 단, 이미지 태그에 한정됩니다.\\n7. **삭제 및 재생성**: 필요 시 생성된 분석환경을 삭제하여 리소스를 회수하고 새로운 분석환경을 생성할 수 있습니다.\\n\\n### 2. 공용 분석환경 생성\\n1. **리소스 요청**: 분석과제에 소속된 사용자는 사전에 생성된 Jupyter Hub의 접속 URL을 등록하여 공용 분석환경을 생성할 수 있습니다.\\n2. **정보 입력**: [새 환경 추가+] 버튼을 클릭하여 공용 분석환경을 추가합니다.\\n3. **환경 추가**: 정보를 입력한 후 [분석환경 추가] 버튼을 클릭하여 등록합니다.\\n4. **확인**: 등록한 분석환경은 분석환경 페이지에서 확인할 수 있습니다. (분석환경 > 공용분석환경)\\n5. **사용 및 수정**: 생성된 공용 분석환경은 자유롭게 사용할 수 있으며, 이후 수정도 가능합니다. 단, 소속 과제는 고정됩니다.\\n\\n### 3. Inference 영역 분석환경 생성\\n1. **IT 관리자 요청**: Inference 영역의 분석환경은 IT 관리자에게 생성을 요청해야 합니다.\\n2. **기본 사양**: 기본 값은 CPU 0.5 Core, RAM 1GB이며 필요 시 협의하여 증설할 수 있습니다.\\n3. **확인**: 생성된 Inference 영역 공용 분석환경은 분석환경 페이지에서 확인할 수 있습니다. (분석환경 > 개인분석환경)\\n4. **리소스 수정 요청**: 생성 후 리소스 수정 등이 필요할 경우 IT 관리자에게 추가로 요청해야 합니다.\\n\\n이 과정을 통해 사용자는 필요한 분석환경을 생성하고, 데이터 분석 및 모델 배포를 위한 작업을 수행할 수 있습니다.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, Docx2txtLoader\n",
    "# pdf 로더\n",
    "# pip installpypdf\n",
    "# docx 로더\n",
    "# pip install docx2txt\n",
    "loader = DirectoryLoader(path='./', glob='*.docx', loader_cls=Docx2txtLoader)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "len(docs)\n",
    "# print(docs)\n",
    "# print(docs[0].page_content)\n",
    "# print(docs[0].metadata)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# print(len(splits))\n",
    "# print(splits[0])\n",
    "\n",
    "# import re\n",
    "\n",
    "# def clean_ws(text: str) -> str:\n",
    "#     text = text.replace(\"\\u00a0\", \" \")\n",
    "#     return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# for d in docs:\n",
    "#     d.page_content = clean_ws(d.page_content)\n",
    "\n",
    "# print(docs[0].page_content)\n",
    "# print(docs[0].metadata)\n",
    "\n",
    "import re\n",
    "\n",
    "def keep_newlines_reduce_blank_lines(text: str) -> str:\n",
    "    # 특수 공백/개행 정리\n",
    "    text = text.replace(\"\\u00a0\", \" \")      # NBSP -> 일반 공백\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 빈 줄 판정에 방해되는 \"줄 끝 공백\" 제거\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "\n",
    "    # (핵심) 2개 이상 연속 빈 줄 -> \"빈 줄 1개\"로 축소\n",
    "    # 즉, \\n\\n\\n... -> \\n\\n\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "for d in docs:\n",
    "    d.page_content = keep_newlines_reduce_blank_lines(d.page_content)\n",
    "\n",
    "# 이제 분할\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# print(splits[3].page_content)\n",
    "\n",
    "# Indexing (Texts -> Embedding -> Store)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits,\n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "# docs = vectorstore.similarity_search(\"분석환경 생성 과정에 대해서 설명해주세요.\")\n",
    "# print(len(docs))\n",
    "# print(docs[0].page_content)\n",
    "\n",
    "template = '''질문에 따라 답변해주세요:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM\n",
    "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "# Rretriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Combine Documents\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join(doc.page_content for doc in docs)\n",
    "\n",
    "# RAG Chain 연결\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain 실행\n",
    "rag_chain.invoke(\"분석환경 생성 과정에 대해서 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a9198",
   "metadata": {},
   "source": [
    "## RAG -Text Splitter\n",
    "LangChain은 긴 문서를 작은 단위인 청크(chunk)로 나누는 텍스트 분리 도구를 다양하게 지원합니다. \n",
    "텍스트를 분리하는 작업을 `청킹(chunking)`이라고 부르기도 합니다. \n",
    "이렇게 문서를 작은 조각으로 나누는 이유는 `LLM 모델의 입력 토큰의 개수가 정해져 있기 `때문입니다. \n",
    "허용 한도를 넘는 텍스트는 모델에서 입력으로 처리할 수 없게 되는 것입니다. \n",
    "한편, 텍스트가 너무 긴 경우에는 핵심 정보 이외에 불필요한 정보들이 많이 포함될 수 있어서 RAG 품질이 낮아지는 요인이 될 수도 있습니다. \n",
    "핵심 정보가 유지될 수 있는 적절한 크기로 나누는 것이 매우 중요합니다.\n",
    "\n",
    "LangChain이 지원하는 다양한 텍스트 분리기(Text Splitter)는 분할하려는 텍스트 유형과 사용 사례에 맞춰 선택할 수 있는 다양한 옵션이 제공됩니다. 크게 두 가지 차원에서 검토가 필요합니다.\n",
    "\n",
    "1. 텍스트가 어떻게 분리되는지:\n",
    "    텍스트를 나눌 때 각 청크가 독립적으로 의미를 갖도록 나눠야 합니다. \n",
    "    이를 위해 문장, 구절, 단락 등 문서 구조를 기준으로 나눌 수 있습니다.\n",
    "\n",
    "2. 청크 크기가 어떻게 측정되는지:\n",
    "    각 청크의 크기를 직접 조정할 수 있습니다. \n",
    "    LLM 모델의 입력 크기와 비용 등을 종합적으로 고려하여 애플리케이션에 적합한 최적 크기를 결정하는 기준입니다. \n",
    "    예를 들면 단어 수, 문자 수 등을 기준으로 나눌 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45709d95",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>LLM 모델의 입력 토큰 수 제한이란?</summary>\n",
    "\n",
    "> LLM 모델은 **한 번에 처리할 수 있는 전체 입력의 크기**에 제한이 있습니다. 이 입력에는:\n",
    "> \n",
    "> 1. **시스템 프롬프트** (모델에게 주는 지시사항)\n",
    "> 2. **사용자 질문** (당신이 물어보는 내용)\n",
    "> 3. **문맥 정보** (RAG에서 검색된 문서 내용) ← **이 부분이 핵심!**\n",
    "> \n",
    "> 이 **모든 것을 합친 총량**이 모델의 토큰 제한을 넘으면 안 됩니다.\n",
    "> \n",
    "> ## 구체적인 예시\n",
    "> \n",
    "> 예를 들어, GPT-4 모델의 토큰 제한이 8,000토큰이라고 가정하면:\n",
    "> \n",
    "> ```\n",
    "> [시스템 프롬프트] 500토큰\n",
    "> [사용자 질문] \"이 회사의 2023년 매출은?\" 50토큰\n",
    "> [검색된 문서] 100페이지짜리 연간보고서 전체 → 50,000토큰 ❌\n",
    "> ```\n",
    "> \n",
    "> → **문제 발생!** 전체 문서를 넣으면 토큰 제한 초과\n",
    "> \n",
    "> ## RAG에서 Text Splitter가 필요한 이유\n",
    "> \n",
    "> 그래서 RAG는 이렇게 작동합니다:\n",
    "> \n",
    "> 1. **사전에 긴 문서를 작은 청크로 분할** (Text Splitter 사용)\n",
    ">    - 100페이지 보고서 → 500개의 작은 청크로 분할\n",
    ">    \n",
    "> 2. **사용자가 질문하면**\n",
    ">    - 질문과 관련된 청크만 3~5개 정도 검색\n",
    ">    \n",
    "> 3. **LLM에 입력**\n",
    ">    ```\n",
    ">    [시스템 프롬프트] 500토큰\n",
    ">    [사용자 질문] 50토큰\n",
    ">    [검색된 관련 청크 5개] 2,000토큰 ✅\n",
    ">    총 2,550토큰 → 제한 내!\n",
    ">    ```\n",
    "> \n",
    "> ## 핵심 포인트\n",
    "> \n",
    "> - **\"입력 토큰 수\"는 질문만이 아니라, 질문 + 검색된 문서 내용 + 프롬프트를 모두 합친 것**\n",
    "> - RAG에서는 긴 문서를 통째로 넣는 게 아니라, 질문과 관련된 작은 조각들만 선별해서 넣음\n",
    "> - 그래서 **미리** 문서를 작은 청크로 나눠두는 것 (청킹)\n",
    "\n",
    "</deatils>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded12d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharacterTextSplitter\n",
    "# TextLoader 클래스는 특정 파일에서 텍스트를 로드해서 Document 객체로 변환합니다. \n",
    "# 여기서는 'history.txt' 파일에서 텍스트를 로드하고, 로드된 데이터 중에서 첫 번째 Document 객체의 페이지 내용의 길이와 해당 내용을 출력하고 있습니다. \n",
    "# 글자 수는 총 1,234개로 확인됩니다.\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('history.txt')\n",
    "data = loader.load()\n",
    "\n",
    "print(len(data[0].page_content))\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2899554",
   "metadata": {},
   "source": [
    "1. 문서를 개별 문자를 단위로 나누기 (separator=\"\")\n",
    "CharacterTextSplitter 클래스는 주어진 텍스트를 문자 단위로 분할하는 데 사용됩니다. \n",
    "파이썬의 split 함수라고 생각하시면 됩니다. \n",
    "다음 코드에서 적용된 주요 매개변수는 다음과 같습니다:\n",
    "\n",
    "`separator` : `분할된 각 청크를 구분할 때 기준이 되는 문자열`입니다. 여기서는 빈 문자열('')을 사용하므로, 각 글자를 기준으로 분할합니다.\n",
    "`chunk_size` : 각 청크의 최대 길이입니다. 여기서는 500으로 설정되어 있으므로, 최대 500자까지의 텍스트가 하나의 청크에 포함됩니다.\n",
    "`chunk_overlap` : 인접한 청크 사이에 중복으로 포함될 문자의 수입니다. 여기서는 100으로 설정되어 있으므로, 각 청크들은 연결 부분에서 100자가 중복됩니다.\n",
    "`length_function` : 청크의 길이를 계산하는 함수입니다. 여기서는 len 함수가 사용되었으므로, 문자열의 길이를 기반으로 청크의 길이를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d919ea34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13345\n",
      "33.3625\n",
      "32.1125\n",
      "34\n",
      "499 493 497\n"
     ]
    }
   ],
   "source": [
    "# 각 문자를 구분하여 분할\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "print(len(data[0].page_content))\n",
    "print(len(data[0].page_content)/400)\n",
    "print((len(data[0].page_content)-500)/400)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = '',\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(data[0].page_content)\n",
    "# split_text 메소드는 주어진 텍스트(data[0].page_content)를 위에서 설정한 매개변수에 따라 분할하고, 분할된 청크의 리스트를 반환합니다. \n",
    "print(len(texts))\n",
    "print(len(texts[0]), len(texts[1]), len(texts[2]))\n",
    "# len(texts)는 분할된 청크의 총 수를 나타냅니다. 여기서는 3개의 청크로 분할됩니다.\n",
    "\n",
    "# 여기서 중요한 것은 각 청크의 크기가 chunk_size를 초과하지 않으며, 인접한 청크 사이에는 chunk_overlap만큼의 문자가 중복되어 있음을 이해하는 것입니다. \n",
    "# 이렇게 함으로써, 텍스트의 의미적 연속성을 유지하면서도 큰 데이터를 더 작은 단위로 분할할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0bdd8",
   "metadata": {},
   "source": [
    "2. 문서를 특정 문자열을 기준으로 나누기 (separator=\"문자열\")\n",
    "CharacterTextSplitter 클래스의 separator 매개변수를 줄바꿈 문자('\\n')로 설정하는 예제입니다. \n",
    "이렇게 하면 각 청크를 나누는 기준을 줄바꿈 문자로 설정하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fadd7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "485 478 500\n"
     ]
    }
   ],
   "source": [
    "# 줄바꿈 문자를 기준으로 분할\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(data[0].page_content)\n",
    "\n",
    "print(len(texts))\n",
    "print(len(texts[0]), len(texts[1]), len(texts[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af79d1c",
   "metadata": {},
   "source": [
    "### RecursiveCharacterTextSplitter\n",
    "`RecursiveCharacterTextSplitter` 클래스는 `텍스트를 재귀적으로 분할하여 의미적으로 관련 있는 텍스트 조각들이 같이 있도록 하는 목적`으로 설계되었습니다. \n",
    "이 과정에서 문자 리스트(['\\n\\n', '\\n', ' ', ''])의 문자를 순서대로 사용하여 텍스트를 분할하며, 분할된 청크들이 설정된 chunk_size보다 작아질 때까지 이 과정을 반복합니다. \n",
    "여기서 chunk_overlap은 분할된 텍스트 조각들 사이에서 중복으로 포함될 문자 수를 정의합니다. \n",
    "length_function = len 코드는 분할의 기준이 되는 길이를 측정하는 함수로 문자열의 길이를 반환하는 len 함수를 사용한다는 의미입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b03577d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(data[0].page_content)\n",
    "\n",
    "len(texts)\n",
    "# texts = text_splitter.split_text(data[0].page_content) 코드는 data[0].page_content에서 첫 번째 문서의 내용을 RecursiveCharacterTextSplitter를 사용하여 분할하고, 결과를 texts 변수에 할당합니다. \n",
    "# data 리스트에서 첫 번째 문서의 내용을 기반으로 분할 작업을 수행하게 됩니다. \n",
    "# len(texts)는 분할된 텍스트 조각들의 총 수를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50fdeb",
   "metadata": {},
   "source": [
    "### 토큰 수를 기준으로 텍스트 분할 (Tokenizer 활용)\n",
    "대규모 언어 모델(LLM)을 사용할 때 모델이 처리할 수 있는 토큰 수에는 한계가 있습니다. \n",
    "입력 데이터를 모델의 제한을 초과하지 않도록 적절히 분할하는 것이 중요합니다. \n",
    "이때 LLM 모델에 적용되는 토크나이저를 기준으로 텍스트를 토큰으로 분할하고, 이 토큰들의 수를 기준으로 텍스트를 청크로 나누면 모델 입력 토큰 수를 조절할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f2a88df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=200,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "len(docs)\n",
    "\n",
    "#OpenAI API의 경우 tiktoken 라이브러리를 통해 해당 모델에서 사용하는 토크나이저를 기준으로 분할할 수 있습니다. \n",
    "# CharacterTextSplitter.from_tiktoken_encoder 메서드는 글자 수 기준으로 분할할 때 tiktoken 토크나이저를 기준으로 글자 수를 계산하여 분할합니다. \n",
    "# 여기서 encoding_name='cl100k_base'는 텍스트를 토큰으로 변환하는 인코딩 방식을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318cc539",
   "metadata": {},
   "source": [
    "## RAG - Embedding\n",
    "임베딩(Embedding)은 텍스트 데이터를 숫자로 이루어진 벡터로 변환하는 과정을 말합니다. \n",
    "이러한 벡터 표현을 사용하면, `텍스트 데이터를 벡터 공간 내에서 수학적으로 다룰 수 있게 되며, 이를 통해 텍스트 간의 유사성을 계산하거나, 텍스트 데이터를 기반으로 하는 다양한 머신러닝 및 자연어 처리 작업을 수행`할 수 있습니다. \n",
    "임베딩 과정은 텍스트의 의미적인 정보를 보존하도록 설계되어 있어, 벡터 공간에서 가까이 위치한 텍스트 조각들은 의미적으로도 유사한 것으로 간주됩니다.\n",
    "\n",
    "- 임베딩의 주요 활용 사례:\n",
    "    `의미 검색(Semantic Search)`\n",
    "        벡터 표현을 활용하여 의미적으로 유사한 텍스트를 검색하는 과정으로, 사용자가 입력한 쿼리에 대해 가장 관련성 높은 문서나 정보를 찾아내는 데 사용됩니다.\n",
    "    `문서 분류(Document Classification)`\n",
    "        임베딩된 텍스트 벡터를 사용하여 문서를 특정 카테고리나 주제에 할당하는 분류 작업에 사용됩니다.\n",
    "    `텍스트 유사도 계산(Text Similarity Calculation)`\n",
    "        두 텍스트 벡터 사이의 거리를 계산하여, 텍스트 간의 유사성 정도를 정량적으로 평가합니다.\n",
    "- 임베딩 모델 제공자:\n",
    "    `OpenAI`\n",
    "        GPT와 같은 언어 모델을 통해 텍스트의 임베딩 벡터를 생성할 수 있는 API를 제공합니다.\n",
    "    `Hugging Face`\n",
    "        Transformers 라이브러리를 통해 다양한 오픈소스 임베딩 모델을 제공합니다.\n",
    "    `Google`\n",
    "        Gemini, Gemma 등 언어 모델에 적용되는 임베딩 모델을 제공합니다.\n",
    "- 임베딩 메소드:\n",
    "    `embed_documents`\n",
    "        이 메소드는 문서 객체의 집합을 입력으로 받아, 각 문서를 벡터 공간에 임베딩합니다. \n",
    "        주로 대량의 텍스트 데이터를 배치 단위로 처리할 때 사용됩니다.\n",
    "    `embed_query`\n",
    "        이 메소드는 단일 텍스트 쿼리를 입력으로 받아, 쿼리를 벡터 공간에 임베딩합니다. \n",
    "        주로 사용자의 검색 쿼리를 임베딩하여, 문서 집합 내에서 해당 쿼리와 유사한 내용을 찾아내는 데 사용됩니다.\n",
    "\n",
    "임베딩은 텍스트 데이터를 머신러닝 모델이 이해할 수 있는 형태로 변환하는 핵심 과정입니다.\n",
    "다양한 자연어 처리 작업의 기반이 되는 중요한 작업입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2a06a8",
   "metadata": {},
   "source": [
    "### OpenAIEmbeddings\n",
    "`OpenAIEmbeddings` 클래스는 OpenAI의 API를 활용하여, 각 문서를 대응하는 임베딩 벡터로 변환합니다. \n",
    "langchain_openai 라이브러리에서 OpenAIEmbeddings 클래스를 직접 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfae96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a79b0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1536)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        '안녕하세요!',\n",
    "        '어! 오랜만이에요',\n",
    "        '이름이 어떻게 되세요?',\n",
    "        '날씨가 추워요',\n",
    "        'Hello LLM!'\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0e8f7",
   "metadata": {},
   "source": [
    "`embed_documents` 메소드는 입력 받은 5개의 문서 객체를 각각 별도의 벡터로 임베딩합니다.\n",
    "`embeddings` 변수에는 각 텍스트에 대한 벡터 표현을 담고 있는 리스트가 할당됩니다. \n",
    "len(embeddings)는 입력된 텍스트 리스트의 개수와 동일하며, 이는 임베딩 과정을 거친 문서의 총 수를 나타냅니다.\n",
    "\n",
    "len(embeddings[0])는 첫 번째 문서의 벡터 표현의 차원을 나타냅니다. \n",
    "일반적으로 이 차원 수는 선택된 모델에 따라 정해지며, 모든 임베딩 벡터는 동일한 차원을 가집니다. \n",
    "OpenAI의 임베딩 모델을 사용할 경우 임베딩 벡터의 차원은 1536이라는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4300d205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01041258405894041, -0.01355851348489523, -0.006538722664117813, -0.018673023208975792, -0.018280573189258575, 0.016685454174876213, -0.009216244332492352, 0.003937159199267626, -0.0074185701087117195, 0.0100644426420331, 0.011760839261114597, -0.006709628272801638, -0.02540796995162964, -0.02252156473696232, -0.004892964847385883, -0.021761983633041382, 0.025281373411417007, -0.01764758862555027, 0.00793128740042448, -0.017837483435869217, -0.008038894273340702, -0.01931866630911827, 0.005560762714594603, -0.008747836574912071, 0.0007754051475785673, 0.007937616668641567, 0.0012129552196711302, -0.017698228359222412, 0.017964079976081848, -0.028231078758835793, 0.012134299613535404, 0.006212736014276743, -0.015444804914295673, 0.000377218791982159, 0.009545396082103252, -0.003971973434090614, 0.0018815443618223071, -0.009456777945160866, 0.0034845760092139244, -0.003680800786241889, 0.009804919362068176, -0.016406940296292305, 0.004165033344179392, -0.017698228359222412, -0.0032820210326462984, 0.02805384248495102, 0.0037440992891788483, -0.036130715161561966, 0.0012849571648985147, 0.04415694996714592, 0.019989628344774246, 0.0063393330201506615, -0.0127482945099473, -0.02878810465335846, -0.00431062001734972, 0.00023796234745532274, -0.017634928226470947, 0.02220507152378559, -0.006279199384152889, -0.0412452258169651, -0.005029056221246719, 0.01419149711728096, -0.013090105727314949, 0.021280916407704353, -0.004927779082208872, -0.0045416587963700294, 0.007235005032271147, 0.0031665016431361437, -0.02663595974445343, 0.013001487590372562, -0.00753250764682889, 0.023268485441803932, -0.01805269904434681, -0.024129342287778854, 0.015457464382052422, 0.010051783174276352, -0.022141773253679276, 0.015508103184401989, -0.03496602550148964, -0.0030050906352698803, 0.028940020129084587, -0.03937159478664398, 0.00684255501255393, 0.005940553266555071, 0.016812050715088844, 0.018191954120993614, -0.02439519576728344, 0.002486043842509389, -0.019293347373604774, -0.026256168261170387, 0.008893422782421112, 0.01882493868470192, -0.013001487590372562, 0.02097708359360695, -0.019508561119437218, 0.011798818595707417, 0.00711473822593689, 0.02992747537791729, -0.013495215214788914, -0.014140858314931393, 0.003532049711793661, -0.012773613445460796, -0.016533536836504936, -0.0039181699976325035, -0.0265093632042408, 7.16557478881441e-05, 0.007943946868181229, -0.01271031517535448, 0.020154204219579697, -0.012976168654859066, -0.0034624214749783278, 0.02025548182427883, -0.014520648866891861, -0.03805498778820038, 0.009304862469434738, 0.01985037326812744, 0.008659218437969685, -0.0015872069634497166, 0.0014155100798234344, -0.023964766412973404, 0.0057538230903446674, 0.02939576841890812, 0.008228790014982224, 0.0010214776266366243, 0.007462879177182913, -0.026737237349152565, -0.010235348716378212, -0.020344099029898643, 0.001318188849836588, -0.00021323641703929752, 0.06729883700609207, 0.020230162888765335, 0.017470354214310646, 0.0029117255471646786, -0.01319138240069151, 0.004351763520389795, 9.351346670882776e-05, 0.017508331686258316, -0.032636646181344986, -0.011165834963321686, 0.0048391614109277725, 0.016963966190814972, -0.004066920839250088, -0.026205530390143394, 0.007133727427572012, 0.009804919362068176, 0.03377601504325867, -0.010988598689436913, 0.01157094445079565, -0.004136549308896065, -0.015356187708675861, -0.0022993136662989855, 0.036333270370960236, -0.0038485415279865265, 0.012260896153748035, 0.022458266466856003, 0.0023388753179460764, 0.006848884746432304, -0.016242364421486855, -0.010241677984595299, -0.016217045485973358, -0.01486246008425951, 0.025762440636754036, -0.0265093632042408, -0.0018578075105324388, 0.0362573117017746, 0.003160171676427126, 0.0127482945099473, -0.0029576169326901436, -0.003930829465389252, -0.005946883000433445, -0.008652889169752598, -0.02117963880300522, -0.004234661813825369, -0.028813423588871956, 0.0012200763449072838, 0.0031712488271296024, 0.0017043089028447866, -0.018432488664984703, -0.018837599083781242, -0.04051096364855766, 0.007722402457147837, 0.016255024820566177, 0.009418799541890621, 0.010564499534666538, 0.010146730579435825, 0.015508103184401989, 0.009931515902280807, 0.01564735919237137, -0.007481868844479322, 0.008266768418252468, 0.02691447176039219, 0.022103795781731606, -0.008013575337827206, -0.6445294618606567, -0.019495900720357895, 0.028940020129084587, 0.017596950754523277, 0.0015674261376261711, 0.007716072723269463, -0.005785472225397825, 0.02683851309120655, -0.013001487590372562, 0.05327191948890686, 0.00017931245383806527, -0.006434280425310135, -0.017596950754523277, 0.003883355762809515, 0.0045195044949650764, -0.016128428280353546, 2.4589937311247922e-05, 0.006595691200345755, 0.005788636859506369, 0.005234776064753532, -0.017445033416152, 0.007785700727254152, -0.017381735146045685, -0.010077102109789848, 0.02399008721113205, 0.007386920973658562, -0.006177922245115042, 0.01455862820148468, -0.013837026432156563, 0.04985380545258522, -0.009798589162528515, 0.02301529236137867, 0.00198282185010612, 0.0033864635042846203, 0.04585334658622742, 0.004585967864841223, 0.005693689454346895, 0.03073769435286522, 0.023597637191414833, 0.05939920246601105, -0.003095290856435895, -0.023471040651202202, 0.01455862820148468, -0.025015519931912422, 0.024825625121593475, 0.003915004897862673, -0.014507989399135113, 0.008387035690248013, -0.015318208374083042, -0.004301125183701515, -0.020635273307561874, -0.0011464919662103057, 0.031168123707175255, 0.010640458203852177, -0.004377082921564579, 0.0014582364819943905, 0.016685454174876213, -0.008709857240319252, 0.01075439527630806, 0.0110772168263793, 0.010032793506979942, 0.01321670226752758, -0.01073540560901165, -0.01301414705812931, -0.03952350839972496, 0.03967542573809624, -0.018166635185480118, 0.006054490339010954, -0.026686597615480423, -0.020230162888765335, -0.011469666846096516, -0.011817808263003826, -0.005652545485645533, 0.01152663491666317, 0.006652659736573696, 0.02073654904961586, 0.03501666337251663, -0.005665204953402281, -0.010172049514949322, 0.0042979600839316845, 0.00554177351295948, 0.007298303302377462, -0.008038894273340702, 0.00732995243743062, 0.010811363346874714, -0.013204042799770832, -0.029041297733783722, -0.032915160059928894, -0.00022985224495641887, -0.005741163156926632, 0.044384825974702835, 0.020052926614880562, 0.018065357580780983, -0.021698685362935066, 0.0010523356031626463, 0.015090334229171276, -0.015014376491308212, 0.006918513216078281, 0.015280229039490223, -0.004177693277597427, 0.011697540991008282, -0.004104900173842907, 0.012355844490230083, -0.0027772164903581142, 0.006060820072889328, 0.014735863544046879, -0.037675198167562485, 0.002435405272990465, 0.016976626589894295, -0.017837483435869217, 0.025521907955408096, -0.0015508102951571345, -0.013115424662828445, 0.009020019322633743, 0.013761067762970924, -0.020647931843996048, -0.006389971822500229, 0.02277475781738758, 0.02000228874385357, 0.0023546998854726553, 0.02163538709282875, -0.015887893736362457, 0.022698799148201942, 0.015520763583481312, 0.003563698846846819, 0.019711116328835487, -0.010703756473958492, -0.010665777139365673, -0.02972492016851902, -0.008804804645478725, -0.0055765872821211815, -0.014128198847174644, -0.021217618137598038, -0.034383680671453476, 0.010849342681467533, -0.008817464113235474, 0.03400389105081558, -0.019128771498799324, 0.02825639769434929, 0.002819943008944392, -0.0053487131372094154, 0.012273556552827358, 0.007127397693693638, 0.0013933556620031595, 0.01075439527630806, -0.0010760724544525146, -0.022129114717245102, 0.0026094757486134768, 0.01683736965060234, 0.008950390852987766, -0.006010181270539761, 0.019204728305339813, -0.0017628599889576435, 0.0037156150210648775, 0.013545854017138481, 0.011197484098374844, -0.0024623069912195206, -0.01449532900005579, -0.024838285520672798, -0.01757162995636463, 0.010456892661750317, 0.013178722932934761, -0.011172164231538773, -0.007424900308251381, 0.006345662754029036, -0.062437523156404495, -0.026534682139754295, 0.0074881985783576965, -0.01209632121026516, -0.023445719853043556, -0.00790596753358841, -0.00791229773312807, -0.013634471222758293, -0.004358093719929457, 0.0001631911436561495, -0.014938417822122574, 0.003924499731510878, -0.00908964779227972, -0.01234318409115076, -0.023597637191414833, 0.005250600632280111, -0.0051271687261760235, -0.009057998657226562, 0.018926216289401054, 0.03281388059258461, 0.021091021597385406, -0.0022281031124293804, 0.029142575338482857, -0.022673480212688446, 0.004573307931423187, 0.021686024963855743, 0.02020484395325184, -0.016280343756079674, -0.0011726025259122252, -0.0058677601628005505, -0.03496602550148964, -0.0015832508215680718, 0.026078933849930763, 0.025673823431134224, 0.024952221661806107, 0.02102772332727909, 0.0018372355261817575, 0.025749782100319862, -0.00906432792544365, -0.010798703879117966, -0.005522783845663071, 0.005098684690892696, -0.023774871602654457, 0.021394852548837662, -0.007057769689708948, 0.003956148866564035, 0.0072603244334459305, 0.00019375239207874984, 0.017090562731027603, -0.00512400409206748, 0.017293117940425873, -0.023281143978238106, -0.0022866539657115936, -0.026661278679966927, 0.006310848519206047, -0.0005146949551999569, 0.0048676454462111, -0.024547113105654716, -0.005639886017888784, -0.00628236448392272, 0.00807054340839386, 0.030281946063041687, 0.030155349522829056, 0.007608465384691954, -0.012159619480371475, -0.029421087354421616, 0.013469895347952843, 0.005399351939558983, 0.023281143978238106, -0.0037472641561180353, -0.0028626692947000265, 0.006019676104187965, -0.009773270227015018, 0.006291859317570925, -0.002386349020525813, -0.007140057627111673, 0.013583832420408726, 0.014913098886609077, -0.012672335840761662, 0.007051439955830574, 0.025952337309718132, 0.01581193506717682, 0.014609266072511673, 0.0028009533416479826, 0.03678901866078377, -0.031573232263326645, -0.00031589847640134394, 0.008469323627650738, -0.02196453884243965, -0.007500858046114445, -0.04256183281540871, 0.01154562458395958, 0.0036997904535382986, 0.028003204613924026, 0.028357675299048424, 0.016001831740140915, -0.0012034605024382472, 0.0047252243384718895, -0.009121296927332878, 0.022293690592050552, 0.022470925003290176, 0.012305205687880516, -0.03797902911901474, -0.002658532001078129, 0.010950620286166668, -0.019508561119437218, -0.01683736965060234, 0.01602715067565441, -0.03838413953781128, -0.004475195426493883, -0.020799847319722176, 0.01716652140021324, -0.035928159952163696, 0.008817464113235474, 0.015761297196149826, 0.0005957960383966565, -0.029674280434846878, -0.0017612774390727282, 0.016685454174876213, 0.01898951455950737, -0.025180095806717873, -0.027598094195127487, -0.0017232984537258744, -0.020103566348552704, 0.021407512947916985, 0.0030731363222002983, 0.016001831740140915, 0.00036258104955777526, -0.005089189857244492, -0.006174757145345211, -0.030687054619193077, 0.02602829411625862, -0.003880190895870328, 0.02191389910876751, 0.021293574944138527, 0.0083933649584651, 0.003807397559285164, -0.010020134039223194, -0.022838056087493896, -0.021559428423643112, 0.03025662712752819, 0.003513060277327895, -0.0043201143853366375, 0.018153976649045944, -0.008380705490708351, -0.011976053938269615, 0.009463108144700527, -0.018445149064064026, -0.015963852405548096, -0.0161031074821949, 0.007981926202774048, -0.025673823431134224, -0.00732995243743062, 0.035117942839860916, -0.0033611441031098366, 0.00642478559166193, -0.009513746947050095, -0.017457693815231323, -0.014001602306962013, 0.03261132538318634, -0.026332126930356026, 0.017495673149824142, -0.0032661964651197195, -0.011450677178800106, 0.011355729773640633, -0.021825281903147697, -0.01582459546625614, -0.024952221661806107, -0.014925758354365826, -0.008336396887898445, -0.017951421439647675, -0.012159619480371475, 0.009709971956908703, 0.025597864761948586, 0.010697426274418831, 0.022230392321944237, -0.011551954783499241, -0.0032630315981805325, 0.023648275062441826, -0.009121296927332878, 0.0063393330201506615, 0.006019676104187965, 0.015052354894578457, 0.01850844733417034, 0.020875805988907814, -0.011444346979260445, -0.003908675163984299, 0.007469208911061287, -0.019951649010181427, -0.02542063035070896, -0.006276034750044346, -0.0005344757228158414, -0.011140515096485615, 0.007494528312236071, 0.07367931306362152, -0.013938304036855698, 0.01601449027657509, -0.0026965111028403044, 0.006747607607394457, 0.035801563411951065, 0.01630566269159317, 0.008728846907615662, 0.0033611441031098366, -0.0105708297342062, 0.0034940706100314856, -0.005763317458331585, 0.01760960929095745, -0.003566863713786006, -0.004348598886281252, 0.022458266466856003, 0.013634471222758293, -0.026053613051772118, -0.003082631155848503, 0.011494985781610012, -0.00014578408445231616, -0.0019923164509236813, -0.01735641621053219, -0.005544938147068024, -0.0032013156451284885, 0.009690982289612293, -0.030914928764104843, -0.01577395759522915, -0.005529113579541445, -0.022192412987351418, -0.01158993411809206, -0.016090448945760727, -0.006234890781342983, -0.009608694352209568, 0.001145700691267848, -0.010115081444382668, -0.01577395759522915, -0.02992747537791729, -0.0011164252646267414, -0.00753250764682889, 0.0110772168263793, 0.020065587013959885, -0.016685454174876213, -0.0024749666918069124, 0.02602829411625862, -0.012577388435602188, -0.00758314598351717, 0.020685911178588867, -0.021901240572333336, -0.0022819065488874912, 0.008842783980071545, 0.00619058171287179, -0.0014851383166387677, -0.01171020045876503, 0.021939219906926155, 0.028762783855199814, 0.022597521543502808, 0.026078933849930763, -0.013900324702262878, 3.24898719554767e-05, -0.006734947673976421, 0.0053550428710877895, 0.005165148060768843, -0.021458150818943977, 0.015609380789101124, 0.009399809874594212, -0.031573232263326645, -0.0071527170948684216, -0.014280115254223347, -0.010798703879117966, 0.016406940296292305, -0.005180972628295422, 0.014634585939347744, 0.006101964041590691, 0.016356302425265312, 0.031902384012937546, -0.033522821962833405, 0.014685224741697311, -0.0057823071256279945, 0.015444804914295673, 0.012653346173465252, -0.008038894273340702, 0.018432488664984703, -0.005915233865380287, -0.01533086784183979, 0.00419984757900238, -0.04033372923731804, 0.025952337309718132, 0.018533766269683838, -0.006633670534938574, 0.012248236685991287, -0.016204385086894035, -0.020964423194527626, 0.0032472070306539536, 0.0017549475887790322, -0.011950734071433544, 0.0033674738369882107, 0.0003582292702049017, -0.0017945091240108013, 0.010317636653780937, -0.011026578024029732, -0.03818158432841301, 0.02204049564898014, -0.013786387629806995, -0.04514440521597862, -0.02805384248495102, -0.032307494431734085, -0.010235348716378212, -0.008747836574912071, -0.022344328463077545, -0.03605475649237633, -0.025724463164806366, 0.0027787990402430296, -0.022876035422086716, 0.010235348716378212, 0.0005625643534585834, 0.004348598886281252, -0.039017122238874435, -0.0030731363222002983, 0.008292088285088539, -0.019875692203640938, 0.0010341373272240162, 0.012805262580513954, 0.05306936427950859, 0.017634928226470947, 0.01671077311038971, 0.009317521937191486, 0.009311191737651825, -0.00146614876575768, -0.00753250764682889, 0.003528884844854474, -0.021331554278731346, 0.006133613176643848, -0.023876149207353592, 0.0031206102576106787, 0.015343527309596539, 0.015672679990530014, 0.007937616668641567, 0.019571859389543533, -0.009304862469434738, 0.0161031074821949, 0.009266883134841919, -0.022622842341661453, 0.0006080601015128195, -0.00807054340839386, 0.002193288877606392, -0.032104939222335815, -0.008121182210743427, -0.0022803242318332195, 0.010697426274418831, -0.016356302425265312, -0.013431916944682598, 0.0029212203808128834, 0.02025548182427883, 0.016115767881274223, 0.013077445328235626, -0.026078933849930763, 0.0052854148671031, 0.029952794313430786, 0.014014261774718761, -0.01735641621053219, -0.000404911843361333, 0.008545281365513802, 0.00454482389613986, 0.003905510064214468, 0.026332126930356026, 0.03577624633908272, -0.003240877063944936, 0.0022027837112545967, -0.004155538976192474, 0.02159740775823593, -0.025306692346930504, -0.03382665663957596, -0.012919199652969837, -0.010526521131396294, -0.02602829411625862, -0.01075439527630806, -0.013875004835426807, -0.018356529995799065, 0.013229361735284328, -0.014849800616502762, -0.019976969808340073, 0.013533193618059158, -0.02419264242053032, -0.028028523549437523, -0.004266310948878527, 0.024812964722514153, 0.030560458078980446, -0.010963279753923416, 0.016609495505690575, 0.019673136994242668, -0.004696739837527275, -0.02931980974972248, 0.00791229773312807, -0.009944175370037556, -0.00961502455174923, 0.01915409043431282, 0.009083317592740059, -0.022673480212688446, -0.01172919012606144, 0.0010879408800974488, 0.016052469611167908, -0.01707790419459343, -0.020546654239296913, 0.024369876831769943, 0.015596721321344376, -0.006469094660133123, -0.000247852731263265, 0.004585967864841223, -0.03134535998106003, 0.037953708320856094, 0.01136838924139738, -0.016001831740140915, -0.015394166111946106, -0.014583947136998177, 0.006646330002695322, 0.006038665771484375, -0.015305548906326294, 0.03175046667456627, 0.027598094195127487, -0.011627912521362305, -0.004582802765071392, -0.0002421163080725819, -0.007545167114585638, 0.0122419074177742, -0.011836796998977661, 0.03987798094749451, -0.028661508113145828, 0.02638276480138302, 0.01671077311038971, -0.004079580772668123, -0.01910345070064068, 0.004247321281582117, -0.011463336646556854, 0.030838971957564354, -0.01630566269159317, -0.015558741986751556, -0.01008343230932951, 0.015229590237140656, 0.023433061316609383, 0.012336854822933674, -0.009469438344240189, -0.010855672881007195, -0.02321784570813179, -0.018837599083781242, 0.014989056624472141, 0.010564499534666538, 0.0038232223596423864, -0.011197484098374844, -0.033725377172231674, -0.01814131624996662, -0.042308639734983444, -0.013786387629806995, 0.010210028849542141, 0.039346273988485336, -0.016938647255301476, -0.007393251173198223, 0.004117559641599655, 0.02167336642742157, 0.006570371799170971, -0.02090112492442131, 0.010539180599153042, -0.0009202001965604722, -0.01988835074007511, 0.008127512410283089, 0.0008489895262755454, 0.001554766553454101, 0.000207895616767928, 0.006500743795186281, 0.010115081444382668, -0.020078247413039207, 0.008633899502456188, -7.393053238047287e-05, -0.024407856166362762, -0.006472259759902954, 0.0015508102951571345, 0.0013292660005390644, -0.010501201264560223, -0.013204042799770832, 0.03225685656070709, -0.0105708297342062, 0.0013332221424207091, -0.009627684019505978, -0.02919321320950985, 0.01449532900005579, -0.014963737688958645, 0.026281487196683884, 0.014014261774718761, -0.01630566269159317, -0.029370449483394623, -0.010298646986484528, -0.005557598080486059, 0.0005075738881714642, -0.0238255113363266, 0.00546898040920496, -0.0015239085769280791, -0.0010570830199867487, 0.0018277408089488745, -0.029041297733783722, 0.006760267075151205, -0.034383680671453476, -0.018483128398656845, 0.017381735146045685, 0.010178379714488983, -0.002577826613560319, 0.005456320475786924, 0.03980202227830887, 0.006193746812641621, 0.01805269904434681, -0.009095977060496807, -0.006402631290256977, -0.0037472641561180353, -0.03387729451060295, -0.007697083055973053, -0.010931630618870258, -0.0005934223299846053, 0.04114394634962082, 0.02293933369219303, -0.006101964041590691, -0.020837826654314995, 0.018153976649045944, -0.02395210787653923, -0.014128198847174644, 0.008804804645478725, -0.010969609953463078, 0.0004707817279268056, -0.012406482361257076, -0.00521578686311841, 0.026002975180745125, -0.003896015463396907, 0.01138737890869379, -0.009418799541890621, -0.011684881523251534, 0.015102993696928024, 0.015609380789101124, -0.007165377028286457, -0.010032793506979942, -0.01809067837893963, -0.015052354894578457, -0.018153976649045944, 0.017305778339505196, -0.022749438881874084, 0.03577624633908272, -0.013242021203041077, -0.013001487590372562, 0.005313899368047714, -0.0019258533138781786, -0.018736321479082108, 0.001758112572133541, 0.032839201390743256, 0.0032503718975931406, -0.003101620590314269, 0.001197921927087009, -0.007380591239780188, 0.008140171878039837, -0.0018894566455855966, 0.025445949286222458, -0.0028262727428227663, 0.030914928764104843, 0.006621010601520538, -0.017381735146045685, 0.026939790695905685, -0.026534682139754295, 0.013697769492864609, -0.0032155578956007957, -0.010615138337016106, 0.014064900577068329, -0.015609380789101124, 0.005506959278136492, -0.024179982021450996, -0.0006187416729517281, -0.017495673149824142, 0.009437788277864456, 0.005747492890805006, -0.006408961024135351, 0.0027645567897707224, -0.007412240374833345, 0.010887322016060352, -0.026787875220179558, 0.007665433920919895, -0.00793128740042448, -0.01122280303388834, -0.027674052864313126, 0.03721944987773895, -0.011343070305883884, -0.013938304036855698, -0.0018799619283527136, -0.000312535761622712, -0.009621353819966316, 0.007228675298392773, -0.0328645184636116, 0.026737237349152565, -0.016419600695371628, 0.010963279753923416, 0.06390604376792908, -0.01687534898519516, 8.772760338615626e-05, 0.02309124916791916, -0.01193174533545971, -0.0017264633206650615, 0.02200251817703247, 0.2315201610326767, -0.020926445722579956, -0.010450562462210655, 0.032307494431734085, -0.001887874212116003, 0.014710543677210808, 0.007855328731238842, -0.00304939947091043, 0.014204156585037708, 0.024952221661806107, -0.025180095806717873, -0.00044664921006187797, -0.006621010601520538, -0.005054375622421503, -0.0023546998854726553, -0.028357675299048424, -0.018559085205197334, 0.013773728162050247, -0.029446406289935112, -0.01931866630911827, -0.0006772927008569241, -0.010526521131396294, -0.017457693815231323, -0.005253765732049942, 0.021901240572333336, -0.001031763618811965, 0.0008648141520097852, 0.013482555747032166, -0.001222450053319335, -0.00041223070002160966, -0.016090448945760727, 0.0026553671341389418, 0.005848770495504141, -0.003082631155848503, -0.003655481617897749, -0.013609152287244797, 0.031244080513715744, 0.01891355589032173, 0.03914371877908707, 0.02195187844336033, 0.003880190895870328, -0.012463451363146305, 7.749106589471921e-05, -0.005769647657871246, -0.005649380385875702, 0.000800724548753351, -0.006747607607394457, -0.029471727088093758, -0.03207961842417717, 0.029750239104032516, -0.0194705817848444, -0.01305212639272213, 0.006791916210204363, 0.06213369220495224, 0.0027139182202517986, 0.004930943716317415, 0.0006333794444799423, -0.008532621897757053, -0.013824366964399815, 0.011330409906804562, 0.007209685631096363, 0.031978342682123184, -0.025230735540390015, 0.033446863293647766, -0.023116568103432655, -0.0032661964651197195, -0.03129471838474274, 0.009513746947050095, -0.0033864635042846203, -0.004427721723914146, -0.006576701998710632, 0.0007659104303456843, 0.015090334229171276, -0.01038726419210434, -0.04233395680785179, -0.02216709405183792, 0.039219677448272705, -0.008311077952384949, 0.031902384012937546, 0.034130487591028214, -0.016318323090672493, 0.004858150612562895, 0.0016726597677916288, -0.0031395996920764446, -0.0184578076004982, -0.017584290355443954, 0.007203355897217989, 0.011963394470512867, -0.03744732216000557, -0.016862688586115837, 0.009095977060496807, -0.00927321333438158, -0.0045479885302484035, -0.014280115254223347, 0.025433288887143135, 0.022635500878095627, -0.004003622569143772, 0.004782192874699831, -0.017217159271240234, 0.020141545683145523, -0.015381506644189358, -0.0013648713938891888, -0.011551954783499241, 0.01736907660961151, -0.0033959581051021814, 0.0065893614664673805, -0.0038010678254067898, -0.00198282185010612, 0.017584290355443954, -0.009475767612457275, -0.0318770632147789, -0.012121640145778656, 0.006108293775469065, -0.002848427277058363, 0.012052011676132679, 0.003017750335857272, 0.01270398497581482, -0.009032678790390491, -0.0017232984537258744, -0.011558284051716328, 0.00959603488445282, -0.00693117268383503, -0.010216359049081802, 0.006364652421325445, 0.02810448221862316, -0.019875692203640938, -0.008773155510425568, 0.0010270162019878626, 0.007355271838605404, -0.03980202227830887, 0.023445719853043556, -0.029370449483394623, 0.032104939222335815, -0.021293574944138527, -0.008336396887898445, 0.003658646484836936, 0.009292202070355415, 0.0194705817848444, -0.018153976649045944, 0.0061209537088871, 0.023167207837104797, -0.0025430123787373304, 0.006526063196361065, -0.012014033272862434, -0.01735641621053219, -0.017875462770462036, 0.014520648866891861, -0.023686254397034645, -0.005832945927977562, -0.006962821818888187, -0.019711116328835487, -0.011925415135920048, -0.021382194012403488, 0.008956721052527428, 0.02423061989247799, -0.011824137531220913, -0.03534581512212753, -0.0318770632147789, -0.004911954514682293, 0.02228103019297123, -0.024610411375761032, 8.886499563232064e-05, 0.029269171878695488, -0.03152259439229965, -0.018597064539790154, -0.003867531195282936, -0.15951192378997803, 0.029142575338482857, 0.00419668247923255, -0.026585320010781288, 0.031573232263326645, 0.020141545683145523, 0.02825639769434929, 0.002343622501939535, -0.008779485709965229, 0.017470354214310646, 0.030712375417351723, -0.03375069797039032, -0.008735177107155323, 0.004259981215000153, 0.0054025170393288136, 0.006244385149329901, -0.005317064002156258, -0.0037156150210648775, 0.010741735808551311, 0.01435607299208641, 0.025673823431134224, -0.03377601504325867, 0.02663595974445343, 0.019546540454030037, 0.00827309861779213, 0.0036143376491963863, -0.010621468536555767, 0.009849227964878082, -0.020268142223358154, 0.005456320475786924, -0.015875233337283134, 0.0016030315309762955, 0.010558170266449451, 0.0013870258117094636, 0.03086429089307785, -0.0006539514288306236, -0.0217999629676342, -0.020837826654314995, -0.024205300956964493, 0.011963394470512867, 0.006652659736573696, 0.013710429891943932, 0.019989628344774246, -0.007608465384691954, -0.031218761578202248, -0.007102078292518854, 0.015039695426821709, 0.026281487196683884, 0.0044118971563875675, -0.009589704684913158, 0.003452926641330123, -0.017508331686258316, -0.003551039146259427, 0.016622155904769897, 0.019293347373604774, 0.037548601627349854, 0.009634013287723064, 0.008836453780531883, 0.0025066158268600702, 0.012482441030442715, 0.009349171072244644, 0.0024259104393422604, 0.011336740106344223, 0.004661925602704287, 0.005510123912245035, 0.0018483127932995558, -0.01663481444120407, 0.016368960961699486, -0.009811249561607838, 0.013242021203041077, -0.0018673022277653217, -0.017343755811452866, -0.010564499534666538, -0.03301643580198288, 0.02578776143491268, -0.013444576412439346, -0.024015406146645546, 0.03382665663957596, -0.007184366229921579, 0.00277088675647974, -0.009032678790390491, 0.027750009670853615, -0.022901354357600212, 0.007203355897217989, -0.007133727427572012, 0.009665663354098797, 0.013900324702262878, -0.01490043941885233, -0.01797674037516117, -0.017191840335726738, 0.03164919093251228, -0.01829323172569275, -0.00939347967505455, -0.013900324702262878, 0.02772469073534012, 0.03506730496883392, -0.015216930769383907, 0.005491134710609913, 0.009887207299470901, -0.005222116596996784, -0.01602715067565441, 0.007988255470991135, -0.019938990473747253, 0.015153632499277592, 0.019217388704419136, 0.0007492945878766477, 0.019255368039011955, 0.00874150637537241, 0.01952122151851654, -0.037877753376960754, -0.007393251173198223, 0.006526063196361065, 0.024015406146645546, 0.02731958217918873, 0.0339026115834713, 0.04246055334806442, -0.019166750833392143, -0.023230506107211113, 0.005696854554116726, -0.0023357102181762457, 0.05474044010043144, -0.012412812560796738, -0.03463687375187874, 0.0013340134173631668, -0.0076401145197451115, -0.006105129141360521, -0.052714891731739044, -0.02818043902516365, 0.015457464382052422, 0.001450324198231101, 0.002031878102570772, 0.026863833889365196, 0.021318895742297173, 0.007336282636970282, -0.040359050035476685, 0.0021141660399734974, -0.016482898965477943, -0.016482898965477943, -0.00937449000775814, -0.004528999328613281, 0.025635844096541405, -0.023103909566998482, -0.0007105243275873363, -0.009684652090072632, -3.23415151797235e-05, 0.015508103184401989, 0.006538722664117813, 0.01398894190788269, 0.016520878300070763, -0.030180668458342552, -0.022901354357600212, 0.020318780094385147, -0.003468751208856702, 0.041903529316186905, 0.003354814136400819, 0.010494871996343136, -0.02402806654572487, -0.018850257620215416, -0.01136838924139738, -0.007773041259497404, 0.0011013918556272984, -0.009159276261925697, -0.04334673285484314, 0.012798932380974293, 0.003179161110892892, -0.022217731922864914, 0.01728045754134655, 0.013533193618059158, 0.000883803644683212, -0.032383453100919724, 0.002789876190945506, 0.020027607679367065, -0.005908904131501913, 0.03375069797039032, 0.01419149711728096, -0.017913442105054855, -0.026458723470568657, -0.01268499530851841, -0.05005636066198349, 0.0041618687100708485, 0.05010699853301048, 0.005969037301838398, 0.020242823287844658, 0.021863261237740517, -0.02029346115887165, -0.00022629171144217253, 0.00660835113376379, 0.006459599826484919, -0.028610868379473686, 0.005098684690892696, 0.0013822783948853612, 0.008646558970212936, -0.0007848999230191112, 0.012045682407915592, 0.0062538799829781055, -0.002951286965981126, -0.002122078323736787, 0.028003204613924026, -0.015039695426821709, 0.03334558755159378, -0.02236964739859104, -0.007684423588216305, -0.03245941177010536, -0.020052926614880562, 0.0010760724544525146, -0.01217860821634531, -0.02630680799484253, -0.019863031804561615, 0.0006828312762081623, -0.00243382272310555, 0.01102024782449007, 0.006690639071166515, -0.009570715017616749, -0.012634357437491417, -0.009076988324522972, -0.022458266466856003, -0.0009225739049725235, 0.01091897115111351, -0.0023626121692359447, 0.00870352704077959, -0.001887874212116003, 0.0177741851657629, -0.010469552129507065, 0.004753708373755217, 0.025116797536611557, 0.014609266072511673, -0.010767054744064808, -0.03965010493993759, -0.07570486515760422, 0.022711459547281265, -0.0044340514577925205, -0.03258600831031799, 0.003579523414373398, -0.0008011201280169189, 0.015343527309596539, -0.011001259088516235, 0.0027851287741214037, -0.02378753200173378, -0.006063985172659159, -0.004630276467651129, 0.009285872802138329, 0.0040321070700883865, -0.029750239104032516, -0.035396456718444824, 0.02573712170124054, 0.008159161545336246, 0.01902749389410019, 0.005969037301838398, -0.0022439276799559593, -0.023736892268061638, -0.006785586476325989, 0.01817929558455944, -0.013963622972369194, 0.020394738763570786, -0.009260552935302258, 0.003187073627486825, 0.003427607472985983, 0.003316835267469287, 0.016330981627106667, -0.03362410143017769, 0.019951649010181427, 0.017014605924487114, -0.011305090971291065, -0.0011813060846179724, 0.005734833423048258, 0.01736907660961151, -0.010127740912139416, 0.01691332831978798, -0.033522821962833405, -0.023698914796113968, 0.019508561119437218, -0.0004351763636805117, 0.0048676454462111, 0.014381391927599907, 0.005525948945432901, 0.028813423588871956, 0.036333270370960236, -0.008469323627650738, 0.012431802228093147, 0.026737237349152565, -0.011963394470512867, -0.003101620590314269, -0.0067032985389232635, -0.023736892268061638, 0.012488770298659801, -0.009013689123094082, 0.025597864761948586, -0.03813094645738602, 0.025547226890921593, -0.017128542065620422, 0.009602364152669907, 0.007551496848464012, -0.009437788277864456, 0.0011876359349116683, -0.014583947136998177, -0.004133384209126234, 0.0011195901315659285, -0.029547683894634247, -0.027294261381030083, 0.00018366420408710837, -0.00927321333438158, 0.004285300616174936, 0.005487969610840082, 0.009197254665195942, 0.005035386420786381, 0.003437102073803544, -0.008102192543447018, 0.029294490814208984, -0.004949933383613825, -0.002747149672359228, -0.009975824505090714, -0.02016686461865902, 0.022458266466856003, 0.02016686461865902, -0.01781216450035572, 0.008754165843129158, 0.006054490339010954, 0.003671306185424328, -0.07124865800142288, -0.010323965921998024, -0.0013229361502453685, 0.0008474070928059518, 0.007032450288534164, 0.019825052469968796, 0.02252156473696232, 0.0035985130816698074, 0.039093080908060074, 0.011830467730760574, 0.00906432792544365, 0.002633212599903345, 0.0030541468877345324, 0.004377082921564579, -0.009501087479293346, 0.004528999328613281, -0.007184366229921579, -0.024129342287778854, -0.002177464310079813, 0.02602829411625862, -0.006969152018427849, 0.01041258405894041, -0.016280343756079674, 0.015748636797070503, -0.019913671538233757, -0.014419371262192726, 0.008311077952384949, -0.0071084084920585155, -0.015254910103976727, 0.024610411375761032, -0.00016744399908930063, 0.011843127198517323, 0.01716652140021324, 0.005412011407315731, 0.023534338921308517, -0.006874204147607088, -0.024104023352265358, -0.005525948945432901, 0.007355271838605404, 0.013064785860478878, -0.0075261774472892284, -0.0047473786398768425, -0.021242937073111534, -0.013090105727314949, -0.0041460441425442696, 0.05246169865131378, 0.0039403242990374565, 0.01752099208533764, -0.03142131492495537, 0.07413506507873535, -0.004693575203418732, -0.011381048709154129, 0.009083317592740059, 0.014571287669241428, 0.011976053938269615, 0.016115767881274223, -0.003095290856435895, -0.017394395545125008, -0.011089876294136047, 0.00751351797953248, -0.013330639339983463, -0.013090105727314949, -0.045524194836616516, -0.018065357580780983, -0.00526642519980669, -0.016812050715088844, 0.004642936401069164, -0.03306707367300987, 0.014685224741697311, 0.03309239447116852, 0.0092289038002491, -0.003011420601978898, 0.027268942445516586, -0.01753365248441696, -0.00937449000775814, 0.0007037988980300725, -0.00660835113376379, -0.00791229773312807, -0.020888466387987137, -0.008013575337827206, 0.001250143046490848, -0.030484501272439957, -0.013431916944682598, -0.001989151583984494, -0.009678322821855545, -0.017900781705975533, -0.009311191737651825, -0.0034750811755657196, 0.008330066688358784, 0.021166978403925896, 0.009722631424665451, -0.014318093657493591, 0.0010831935796886683, 0.015457464382052422, -0.007241334766149521, -0.019002174958586693, 0.007216015364974737, -0.002454394707456231]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c9043a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.003640108974650502,\n",
       " -0.024275783449411392,\n",
       " 0.010910888202488422,\n",
       " -0.04110145568847656,\n",
       " -0.004543057177215815]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')\n",
    "embedded_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f001a",
   "metadata": {},
   "source": [
    "`embed_query` 메소드는 단일 쿼리 문자열을 받아 이를 벡터 공간에 임베딩합니다. \n",
    "주로 검색 쿼리나 질문 같은 단일 텍스트를 임베딩할 때 유용하며, 생성된 임베딩을 사용해 유사한 문서나 답변을 찾을 수 있습니다.\n",
    "\n",
    "embedded_query[:5]는 생성된 임베딩 벡터의 처음 5개 원소를 슬라이싱하여 반환합니다. \n",
    "임베딩의 일부 특성을 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c1bfbee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8347793912001632\n",
      "0.8154197762848945\n",
      "0.8844172747319565\n",
      "0.7898703827307417\n",
      "0.7467077657972327\n"
     ]
    }
   ],
   "source": [
    "# 코사인 유사도\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(A, B):\n",
    "  return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "for embedding in embeddings:\n",
    "    print(cos_sim(embedding, embedded_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c3c80",
   "metadata": {},
   "source": [
    "코사인 유사도는 두 벡터 간의 코사인 각을 이용하여 유사성을 측정하는 방법입니다. \n",
    "두 벡터의 방향이 완전히 동일하면 코사인 유사도는 1이 됩니다. \n",
    "90도로 수직이면 0, 반대 방향이면 -1이 됩니다. 이는 텍스트 임베딩과 같이 고차원 공간에서 벡터 간 유사도를 측정하는 데 유용하게 사용됩니다.\n",
    "\n",
    "주어진 cos_sim 함수는 두 벡터 A와 B 사이의 코사인 유사도를 계산합니다. \n",
    "dot(A, B)는 두 벡터의 내적을, norm(A)와 norm(B)는 각각 벡터 A와 B의 노름(크기)을 계산합니다. \n",
    "이 함수는 내적 값과 두 벡터 크기의 곱으로 나눈 값으로 코사인 유사도를 계산합니다.\n",
    "\n",
    "이 예시에서는 앞에서 임베딩 변환한 문서들(embeddings)과 하나의 임베딩된 쿼리(embedded_query) 사이의 코사인 유사도를 계산하여 출력합니다. \n",
    "각 문서 임베딩에 대해 cos_sim 함수를 호출하여, 해당 문서가 쿼리와 얼마나 유사한지를 숫자로 나타냅니다. 유사도가 높은 문서일수록 쿼리와 더 관련이 깊다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7451fb10",
   "metadata": {},
   "source": [
    "### HuggingFaceEmbeddings\n",
    "`sentence-transformers` 라이브러리를 사용하면 HuggingFace 모델에서 사용된 사전 훈련된 임베딩 모델을 다운로드 받아서 적용할 수 있습니다. OpenAI 임베딩 모델을 사용할 때는 API 사용료가 부과되지만, HuggingFace의 오픈소스 기반의 임베딩 모델을 사용하면 요금이 부과되지 않습니다.\n",
    "\n",
    "sentence-transformers 라이브러리를 설치\n",
    "```\n",
    "pip install -U sentence-transformers\n",
    "\n",
    "```\n",
    "\n",
    "HuggingFaceEmbeddings 클래스는 Hugging Face의 트랜스포머 모델을 사용하여 문서 또는 문장을 임베딩하는 데 사용됩니다. \n",
    "다음은 주요 매개변수의 설정 값을 설명합니다.\n",
    "\n",
    "- `model_name='jhgan/ko-sroberta-nli'`\n",
    "  - 사용할 모델을 지정합니다. \n",
    "  - 여기서는 한국어 자연어 추론(Natural Language Inference, NLI)에 최적화된 ko-sroberta 모델을 사용합니다.\n",
    "- `model_kwargs={'device':'cpu'}`\n",
    "  - 모델이 CPU에서 실행되도록 설정합니다. \n",
    "  - GPU를 사용할 수 있는 환경이라면 'cuda'로 설정할 수도 있습니다.\n",
    "- `encode_kwargs={'normalize_embeddings':True}`\n",
    "  - 임베딩을 정규화하여 모든 벡터가 같은 범위의 값을 갖도록 합니다. \n",
    "  - 이는 유사도 계산 시 일관성을 높여줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57c72397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d3/h50bjcxd1nxb6d98lbh8jhgw0000gn/T/ipykernel_289/1536508419.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_model = HuggingFaceEmbeddings(\n",
      "/Users/seohyuktaek/Documents/study/langchain/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'RobertaModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "), model_name='jhgan/ko-sroberta-nli', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "\n",
    "embeddings_model\n",
    "\n",
    "# embeddings_model 을 출력해보면 Pooling 레이어의 word_embedding_dimension 값에서 임베딩 벡터의 크기를 확인할 수 있습니다. \n",
    "# 768차원의 벡터라는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "03ce56e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        '안녕하세요!',\n",
    "        '어! 오랜만이에요',\n",
    "        '이름이 어떻게 되세요?',\n",
    "        '날씨가 추워요',\n",
    "        'Hello LLM!'\n",
    "    ]\n",
    ")\n",
    "len(embeddings), len(embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11f0b668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5899015507255911\n",
      "0.4182630956904064\n",
      "0.7240603990052181\n",
      "0.057026526289353854\n",
      "0.43164171480259006\n"
     ]
    }
   ],
   "source": [
    "embedded_query = embeddings_model.embed_query('첫인사를 하고 이름을 물어봤나요?')\n",
    "\n",
    "for embedding in embeddings:\n",
    "    print(cos_sim(embedding, embedded_query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9932465",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>차원수</summary>\n",
    "\n",
    "> 차원이 다르다고 해서 “1536이 무조건 더 좋다”거나 “768은 나쁘다”는 뜻은 아닙니다. **임베딩 차원(dimension)은 “문장을 숫자 벡터로 표현할 때, 그 벡터가 몇 개의 숫자로 이루어져 있느냐”**를 말해요.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 1) 여기서 말하는 “차원”이 뭐야?\n",
    "> \n",
    "> 임베딩은 문장/문서를 다음처럼 **벡터(숫자 배열)** 로 바꾼 결과입니다.\n",
    "> \n",
    "> - `ko-sroberta-nli` → 각 문장을 **768개 숫자**로 표현 (벡터 길이 768)\n",
    "> - `OpenAIEmbeddings` → 각 문장을 **1536개 숫자**로 표현 (벡터 길이 1536)\n",
    "> \n",
    "> 즉,\n",
    "> - (5, 768)은 “문장 5개, 각 문장은 768차원 벡터”\n",
    "> - (5, 1536)은 “문장 5개, 각 문장은 1536차원 벡터”\n",
    "> \n",
    "> 이 차원 벡터는 RAG에서 주로 **유사도 계산(코사인 유사도 등)** 에 쓰입니다.  \n",
    "> “질문 벡터”와 “문서 청크 벡터”가 가까우면 관련성이 높다고 보고 가져오는 거죠.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 2) 그럼 차원이 크면 더 좋은 거 아니야?\n",
    "> \n",
    "> 꼭 그렇지 않습니다. 품질은 단순히 차원 수로 결정되지 않고, 아래가 더 중요합니다.\n",
    "> \n",
    "> ### 더 중요한 요소들\n",
    "> 1. **모델이 어떤 데이터/목표로 학습되었는지**  \n",
    ">    - 한국어에 강한지\n",
    ">    - “검색용(retrieval)”으로 학습된 모델인지\n",
    ">    - 문장 유사도/의미검색에 특화됐는지\n",
    "> \n",
    "> 2. **도메인 적합성**\n",
    ">    - 네 문서가 법률/의학/코드/사내문서/대화체 등 무엇인지에 따라 달라짐\n",
    "> \n",
    "> 3. **정규화(normalize_embeddings) / distance metric / 인덱스 설정**\n",
    ">    - normalize를 켜면 코사인 유사도 기반 검색에서 안정적인 경우가 많음(지금 설정은 좋은 편)\n",
    "> \n",
    "> 4. **Chunking / Rerank / Hybrid 검색**  \n",
    ">    - 실제 RAG 품질은 임베딩 모델 단독보다 파이프라인 전체 영향이 큼\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 3) 실무적으로 “무엇이 더 좋냐”는 이렇게 판단해\n",
    "> \n",
    "> ### A. 한국어 RAG(온프렘)라면\n",
    "> - `jhgan/ko-sroberta-nli`는 **한국어 문장 의미 유사도** 용도로는 괜찮을 수 있지만,\n",
    "> - RAG “검색”에서는 **retrieval 최적화된 임베딩 모델**이 더 유리한 경우가 많습니다(예: bge 계열, e5 계열 등).\n",
    "> \n",
    "> 즉, “OpenAI(1536) vs ko-sroberta(768)”의 승부가 아니라  \n",
    "> **“검색용으로 강한 임베딩 모델인가?”**가 핵심이에요.\n",
    "> \n",
    "> ### B. OpenAIEmbeddings가 유리한 경우(일반적으로)\n",
    "> - 다양한 언어/도메인에서 평균적으로 성능이 잘 나오는 편\n",
    "> - 다만 **온프렘 목표**라면 비용/프라이버시/네트워크 의존성이 단점\n",
    "> \n",
    "> ### C. 차원 관점에서의 트레이드오프\n",
    "> - **차원↑(1536)**: 표현력 잠재력↑ 가능 / 하지만 저장공간↑, 검색 약간 느려질 수 있음\n",
    "> - **차원↓(768)**: 가볍고 빠름 / 하지만 모델 자체 성능이 더 중요\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 4) “같은 벡터DB에 768과 1536을 섞어도 되나?”\n",
    "> 안 됩니다. 벡터DB에서 유사도 계산은 **차원이 동일해야** 해요.\n",
    "> \n",
    "> - 인덱스(컬렉션) 하나에는 보통 **하나의 임베딩 모델(=고정 차원)** 만 사용합니다.\n",
    "> - 모델을 바꾸면 보통 **전체 문서를 재임베딩 후 재인덱싱** 해야 합니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 5) 결론: 너는 뭘 쓰는 게 좋아?\n",
    "> \n",
    "> 당신이 “온프렘 LLM + 온프렘 RAG”를 하려는 목적이라면:\n",
    "> \n",
    "> - 임베딩도 온프렘으로 가는 게 자연스럽고,\n",
    "> - 그 경우 “한국어 검색”에 강한 **retrieval 특화 임베딩 모델**을 추천하는 쪽이 보통 결과가 좋습니다.\n",
    "> - 단순히 **768 vs 1536**로 고르는 게 아니라, **네 문서/질문에서 검색 정확도를 측정**해서 고르는 게 정답입니다.\n",
    "> \n",
    "> ---\n",
    "> \n",
    "> ## 6) 내가 바로 추천/가이드해줄 수 있게 3가지만 알려줘\n",
    "> 1) 문서 종류: PDF 위주? (논문/매뉴얼/사내규정/코드/대화로그 등)  \n",
    "> 2) 질문 형태: “요약/정의/근거찾기/QA” 중 어떤 게 많아?  \n",
    "> 3) 집 PC에 GPU 있어? 있다면 VRAM 몇 GB?\n",
    "> \n",
    "> 이거 주면 “온프렘 임베딩 모델 후보 2~3개 + 청킹값 + 평가 방법(작게 테스트셋 만들어서 비교)”까지 딱 맞춰서 제안해줄게.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b78605",
   "metadata": {},
   "source": [
    "## RAG - Vector Store\n",
    "벡터 저장소(Vector Store)는 벡터 형태로 표현된 데이터, 즉 임베딩 벡터들을 효율적으로 저장하고 검색할 수 있는 시스템이나 데이터베이스를 의미합니다. 자연어 처리(NLP), 이미지 처리, 그리고 기타 다양한 머신러닝 응용 분야에서 생성된 고차원 벡터 데이터를 관리하기 위해 설계되었습니다. \n",
    "벡터 저장소의 핵심 기능은 대규모 벡터 데이터셋에서 빠른 속도로 가장 유사한 항목을 찾아내는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faac699",
   "metadata": {},
   "source": [
    "### Chroma\n",
    "Chroma는 임베딩 벡터를 저장하기 위한 오픈소스 소프트웨어로, LLM(대규모 언어 모델) 앱 구축을 용이하게 하는 핵심 기능을 수행합니다. \n",
    "Chroma의 주요 특징은 다음과 같습니다:\n",
    "\n",
    "- 임베딩 및 메타데이터 저장\n",
    "  - 대규모의 임베딩 데이터와 이와 관련된 메타데이터를 효율적으로 저장할 수 있습니다.\n",
    "- 문서 및 쿼리 임베딩\n",
    "  - 텍스트 데이터를 벡터 공간에 매핑하여 임베딩을 생성할 수 있으며, 이를 통해 검색 작업이 가능합니다.\n",
    "- 임베딩 검색\n",
    "  - 사용자 쿼리에 기반하여 가장 관련성 높은 임베딩을 찾아내는 검색 기능을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7233f0ec",
   "metadata": {},
   "source": [
    "#### 유사도 기반 검색\n",
    "`Chroma` 벡터 저장소를 사용하여 임베딩된 텍스트 데이터를 저장하고 검색하는 방법을 설명합니다. \n",
    "Chroma 벡터 저장소를 사용하여 대규모 텍스트 데이터셋에서 빠르고 효율적으로 유사도 기반 검색(Similarity search)을 수행할 수 있습니다. \n",
    "구체적인 단계는 다음과 같습니다\n",
    "\n",
    "1. 텍스트 데이터 로드\n",
    "2. 텍스트 분할\n",
    "3. 임베딩 모델 초기화\n",
    "4. Chroma 벡터 저장소 생성\n",
    "5. 유사도 기반 검색 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "84eadfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "조선은 1392년 이성계에 의해 건국되어, 1910년까지 이어졌습니다. 조선 초기에는 세종대왕이 한글을 창제하여 백성들의 문해율을 높이는 등 문화적, 과학적 성취가 이루어졌습니다. 그러나 조선 후기에는 내부적으로 실학의 발전과 함께 사회적 변화가 모색되었으나, 외부로부터의 압력은 점차 커져만 갔습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "loader = TextLoader('history.txt')\n",
    "#TextLoader 클래스를 사용해 history.txt 파일에서 텍스트 데이터를 로드합니다.\n",
    "\n",
    "data = loader.load()\n",
    "# 로드된 데이터는 data 변수에 저장됩니다.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250,\n",
    "    chunk_overlap=50,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "# RecursiveCharacterTextSplitter를 사용하여 로드된 텍스트를 여러 개의 작은 조각으로 분할합니다\n",
    "\n",
    "texts = text_splitter.split_text(data[0].page_content)\n",
    "# 분할된 텍스트는 texts 변수에 저장됩니다.\n",
    "\n",
    "texts[0]\n",
    "# 분할된 텍스트의 첫 번째 조각을 출력합니다.\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# OpenAIEmbeddings 모델을 초기화합니다.\n",
    "# OpenAIEmbeddings를 사용하여 OpenAI 임베딩 모델의 인스턴스를 생성합니다. \n",
    "# 이 단계에서 Huggingface 또는 다른 임베딩 모델을 사용할 수 있습니다.\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "    texts, \n",
    "    embeddings_model,\n",
    "    collection_name = 'history',\n",
    "    # 저장소는 collection_name으로 구분되며, 여기서는 'history'라는 이름을 사용합니다.\n",
    "    persist_directory = './db/chromadb',\n",
    "    # 저장된 데이터는 ./db/chromadb 디렉토리에 저장됩니다.\n",
    "    collection_metadata = {'hnsw:space': 'cosine'}, # l2 is the default\n",
    "    # collection_metadata에서 'hnsw:space': 'cosine'을 설정하여 유사도 계산에 코사인 유사도를 사용합니다.\n",
    ")\n",
    "\n",
    "# Chroma 벡터 저장소를 생성합니다.\n",
    "# Chroma.from_texts 메소드를 사용하여 분할된 텍스트들을 임베딩하고, 이 임베딩을 Chroma 벡터 저장소에 저장합니다.\n",
    "\n",
    "\n",
    "db\n",
    "\n",
    "# 생성된 Chroma 벡터 저장소를 출력합니다.\n",
    "\n",
    "query = '누가 한글을 창제했나요?'\n",
    "# query 변수에 검색 쿼리를 정의합니다.\n",
    "\n",
    "docs = db.similarity_search(query)\n",
    "# db.similarity_search 메소드를 사용하여 저장된 데이터 중에서 쿼리와 가장 유사한 문서를 찾습니다.\n",
    "\n",
    "print(docs[0].page_content)\n",
    "\n",
    "# 유사도 기반 검색을 수행합니다.\n",
    "# 검색 결과를 docs 변수에 저장하고, 가장 유사한 문서의 내용은 docs[0].page_content를 통해 확인합니다.\n",
    "# 이 과정을 통해, 주어진 쿼리('누가 한글을 창제했나요?')에 대해 가장 관련성 높은 텍스트 조각('...세종대왕이 한글을 창제하여...')을 찾아내고 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61aac9",
   "metadata": {},
   "source": [
    "#### MMR\n",
    "최대 한계 관련성(Maximum Marginal Relevance, MMR) 검색 방식은 유사성과 다양성의 균형을 맞추어 검색 결과의 품질을 향상시키는 알고리즘입니다. 이 방식은 검색 쿼리에 대한 문서들의 관련성을 최대화하는 동시에, 검색된 문서들 사이의 중복성을 최소화하여, 사용자에게 다양하고 풍부한 정보를 제공하는 것을 목표로 합니다\n",
    "\n",
    "- MMR의 작동 원리\n",
    "  - MMR은 쿼리에 대한 각 문서의 유사성 점수와 이미 선택된 문서들과의 다양성(또는 차별성) 점수를 조합하여, 각 문서의 최종 점수를 계산합니다. 이 최종 점수에 기반하여 문서를 선택합니다. MMR은 다음과 같이 정의될 수 있습니다\n",
    "  ![MMR 계산식](/Users/seohyuktaek/Documents/study/langchain/SCR-20260123-mokm.png)\n",
    "- MMR의 주요 매개변수\n",
    "  - `query`\n",
    "    - 사용자로부터 입력받은 검색 쿼리입니다.\n",
    "  - `k`\n",
    "    - 최종적으로 선택할 문서의 수입니다. \n",
    "    - 이 매개변수는 반환할 문서의 총 개수를 결정합니다.\n",
    "  - `fetch_k`\n",
    "    - MMR 알고리즘을 수행할 때 고려할 상위 문서의 수입니다. \n",
    "    - 이는 초기 후보 문서 집합의 크기를 의미하며, 이 중에서 MMR에 의해 최종 문서가 k개 만큼 선택됩니다.\n",
    "  - `lambda_mult`\n",
    "    - 쿼리와의 유사성과 선택된 문서 간의 다양성 사이의 균형을 조절합니다.\n",
    "\n",
    "MMR 방식을 사용하면, 검색 결과로 얻은 문서들이 쿼리와 관련성이 높으면서도 서로 다른 측면이나 정보를 제공하도록 할 수 있습니다. \n",
    "이는 특히 정보 검색이나 추천 시스템에서 사용자에게 더 풍부하고 만족스러운 결과를 제공하는 데 도움이 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da896e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data -> Text split\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "loader = PyMuPDFLoader('73da18d672b84bb06ac7e44e47ac48dc.pdf')\n",
    "# PyMuPDFLoader를 사용하여 PDF 파일('323410_카카오뱅크_2023.pdf')에서 텍스트 데이터를 로드합니다. \n",
    "# 이 클래스는 PyMuPDF 라이브러리를 사용하여 PDF 문서의 내용을 추출합니다. \n",
    "# 필요한 경우 pip install pymupdf 명령어로 라이브러리를 설치합니다.\n",
    "data = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "# RecursiveCharacterTextSplitter를 사용하여 문서를 텍스트 조각으로 분할하는 인스턴스를 생성하고 text_splitter.split_documents(data)를 호출하여 로드된 문서 객체를 여러 개의 청크로 분할합니다. \n",
    "# documents 변수에는 모두 145개의 문서 조각으로 분할되어 저장됩니다\n",
    "documents = text_splitter.split_documents(data)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "186091cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x35d972c10>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding -> Upload to Vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "# OpenAIEmbeddings 클래스를 사용하여 임베딩 모델의 인스턴스를 생성합니다. \n",
    "\n",
    "db2 = Chroma.from_documents(\n",
    "    documents, \n",
    "    embeddings_model,\n",
    "    collection_name = 'esg',\n",
    "    # 여기서는 esg라는 컬렉션 이름을 사용하며, 데이터는 ./db/chromadb 디렉토리에 저장됩니다. \n",
    "    persist_directory = './db/chromadb',\n",
    "    collection_metadata = {'hnsw:space': 'cosine'}, # l2 is the default\n",
    "    # collection_metadata를 통해 유사도 검색에 사용될 공간('hnsw:space')을 'cosine'으로 지정하여, 코사인 유사도를 사용합니다.\n",
    ")\n",
    "# Chroma.from_documents 메소드를 사용하여 분할된 문서들을 임베딩하고, 이 임베딩들을 Chroma 벡터 저장소에 저장합니다. \n",
    "db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baf9f6",
   "metadata": {},
   "source": [
    "이 과정을 통해, PDF 문서의 텍스트를 추출하고, 이를 의미적으로 응집력 있는 조각으로 분할한 뒤, 각 조각을 벡터로 변환하여 Chroma 벡터 저장소에 저장하게 됩니다. \n",
    "일반 유사도 기반 검색과 MMR 검색을 비교해보겠습니다. \n",
    "먼저 일반적인 유사도 기반 검색을 수행한 후, 최대 한계 관련성(Maximum Marginal Relevance, MMR) 검색을 사용하는 과정을 살펴봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47914a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1) 실사용량 집계가 가능한 오피스·데이터센터만 측정 대상에 포함\n",
      "환경영향 최소화\n",
      "카카오뱅크는 인터넷전문은행으로 \n",
      "환경경영을 통해 친환경 가치를 실현하고 \n",
      "있습니다. 온실가스 배출권 거래제의 \n",
      "의무대상은 아니지만, 환경영향을 \n",
      "최소화하고 리스크를 관리하기 위해 \n",
      "자발적으로 온실가스 배출량을 산출하고 \n",
      "제3자 검증을 거쳐 공개하고 있습니다. \n",
      "앞으로도 카카오뱅크는 환경 데이터 \n",
      "집계와 탄소 배출량 관리를 통해 환경에 \n",
      "미치는 부정적인 영향을 최소화할 수 있는 \n",
      "방안을 모색해 나가겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 1. 일반적인 유사도 기반 검색\n",
    "query = '카카오뱅크의 환경목표와 세부추진내용을 알려줘?'\n",
    "docs = db2.similarity_search(query)\n",
    "print(len(docs))\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d188d61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재무적인 경로를 분석하여 향후 기후리스크에 대한 선제적이고 체계적인 대응을 위한 준비를 시작하였습니다.\n",
      "카카오뱅크는 투자의사결정 시 환경, 사회, 지배구조 요소가 충분히 고려되도록 투자 가이드라인에 ESG 요소를 포함합니다. 2024년 3월까지 \n",
      "카카오뱅크는 ESG 등급이 우수한 기업과 ESG 목적 채권 등으로 구성된 ESG 펀드에 약 700억 원을 투자하였고 추후 ESG 체크리스트를 개발하는 등 \n",
      "ESG 투자 방식을 고도화할 예정입니다.\n",
      "카카오뱅크는 친환경 건축물 인증제도(LEED)를 받은 판교테크원에 입주하여 용수사용량을 저감하고 고효율 LED 및 자동점등 센서를 사용하여 에너지를 \n",
      "저감했습니다. 고객 대상으로는 교통카드 온라인 충전 서비스를 제공하여 종이영수증 발행을 감소시켰으며, 추후에는 고객의 친환경 활동에 대해 \n",
      "우대금리를 제공하는 상품을 개발할 예정입니다. \n",
      "27~35, 80\n",
      "기후변화 관련 위험과 기회가 사업, 전략 및 재무 계획에 미치는 영향\n",
      "2℃ 이하 시나리오 등 기후변화 관련 시나리오를 고려한 조직 회복탄력성\n",
      "위험관리\n",
      "기후변화 관련 위험을 식별 및 평가하기 위한 프로세스\n",
      "카카오뱅크는 중요 리스크에 기후리스크를 포함한 바 있으며, 이를 근거로 향후 기업 대출 강화 등 포트폴리오 다변화에 따른 기후리스크 평가 시스템을 \n",
      "구축해 나갈 예정입니다. 연 1회 환경영향평가를 실시하여 이해관계자 요구사항을 반영한 주요 리스크와 기회를 식별 및 관리하고 있습니다. 도출된 이슈를 \n",
      "해결하기 위한 목표와 세부 추진계획을 세워 환경영향을 최소화하고 있으며, 관련 지표들을 모니터링하고 있습니다. \n",
      "또한 2023년에는 환경경영시스템 매뉴얼 및 가이드라인을 제정하여 환경성과를 향상 시키고 환경목표를 달성하기 위해 노력하고 있습니다.\n",
      "카카오뱅크는 의무적인 기후 또는 환경 규제(배출권 거래제 등)에 적용을 받지 않고, Scope 1&2로 배출되는 탄소배출량이 배출권 거래제에서 요구하는\n"
     ]
    }
   ],
   "source": [
    "# 다음은 검색 결과 중 가장 유사도가 낮은(또는 마지막에 위치한) 문서의 내용을 출력합니다.\n",
    "print(docs[-1].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "41dbb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1) 실사용량 집계가 가능한 오피스·데이터센터만 측정 대상에 포함\n",
      "환경영향 최소화\n",
      "카카오뱅크는 인터넷전문은행으로 \n",
      "환경경영을 통해 친환경 가치를 실현하고 \n",
      "있습니다. 온실가스 배출권 거래제의 \n",
      "의무대상은 아니지만, 환경영향을 \n",
      "최소화하고 리스크를 관리하기 위해 \n",
      "자발적으로 온실가스 배출량을 산출하고 \n",
      "제3자 검증을 거쳐 공개하고 있습니다. \n",
      "앞으로도 카카오뱅크는 환경 데이터 \n",
      "집계와 탄소 배출량 관리를 통해 환경에 \n",
      "미치는 부정적인 영향을 최소화할 수 있는 \n",
      "방안을 모색해 나가겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 2. MMR 검색\n",
    "# 동일한 쿼리를 사용하여 MMR 검색을 수행합니다. \n",
    "# 여기서는 k=4와 fetch_k=10을 설정하여, 상위 10개의 유사한 문서 중에서 서로 다른 정보를 제공하는 4개의 문서를 선택합니다.\n",
    "mmr_docs = db2.max_marginal_relevance_search(query, k=4, fetch_k=10)\n",
    "print(len(mmr_docs))\n",
    "print(mmr_docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd32af9",
   "metadata": {},
   "source": [
    "#### 백터스토어와 메타데이터를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6f3383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 메타데이터 포함하여 문서 준비\n",
    "# 먼저, 각 문서에 대한 메타데이터를 포함하는 구조를 준비합니다. \n",
    "# Document 클래스를 사용하여 문서 내용과 메타데이터를 함께 관리할 수 있습니다.\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"LangChain은 대규모 언어 모델(LLM)을 사용하는 애플리케이션을 개발하기 위한 프레임워크입니다.\",\n",
    "        metadata={\n",
    "            \"title\": \"LangChain 소개\",\n",
    "            \"author\": \"AI 개발자\",\n",
    "            \"url\": \"http://example.com/langchain-intro\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 데 특화된 데이터베이스 시스템입니다.\",\n",
    "        metadata={\n",
    "            \"title\": \"벡터 데이터베이스 개요\",\n",
    "            \"author\": \"데이터 과학자\",\n",
    "            \"url\": \"http://example.com/vector-db-overview\"\n",
    "        }\n",
    "    )\n",
    "    # 추가 문서들...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e100ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Chroma 백터 스토어에 문서와 메타데이터 저장\n",
    "# Chroma 벡터 스토어에 문서를 저장할 때, from_documents 메서드를 사용하여 문서와 메타데이터를 함께 저장합니다.\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Chroma 벡터 스토어에 문서와 메타데이터 저장\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_db\"  # 벡터 스토어를 디스크에 저장\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8f8bdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내용: LangChain은 대규모 언어 모델(LLM)을 사용하는 애플리케이션을 개발하기 위한 프레임워크입니다.\n",
      "제목: LangChain 소개\n",
      "저자: AI 개발자\n",
      "URL: http://example.com/langchain-intro\n",
      "---\n",
      "내용: 벡터 데이터베이스는 고차원 벡터를 효율적으로 저장하고 검색하는 데 특화된 데이터베이스 시스템입니다.\n",
      "제목: 벡터 데이터베이스 개요\n",
      "저자: 데이터 과학자\n",
      "URL: http://example.com/vector-db-overview\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# 3. 유사성 검색 수행 및 메타데이터 활용\n",
    "query = \"LangChain이란 무엇인가요?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "\n",
    "for doc in results:\n",
    "    print(f\"내용: {doc.page_content}\")\n",
    "    print(f\"제목: {doc.metadata['title']}\")\n",
    "    print(f\"저자: {doc.metadata['author']}\")\n",
    "    print(f\"URL: {doc.metadata['url']}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7979fb",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "FAISS(Facebook AI Similarity Search)는 Facebook AI Research에 의해 개발된 라이브러리로, 대규모 벡터 데이터셋에서 유사도 검색을 빠르고 효율적으로 수행할 수 있게 해줍니다. \n",
    "FAISS는 특히 벡터의 압축된 표현을 사용하여 메모리 사용량을 최소화하면서도 검색 속도를 극대화하는 특징이 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c721ac7",
   "metadata": {},
   "source": [
    "#### 유사도 기반 검색 (Similarity search)\n",
    "FAISS 기반의 벡터 스토어를 생성하고 Huggingface에서 한국어 임베딩 모델을 다운로드 받아서 검색하는 과정을 살펴 보겠습니다. \n",
    "먼저 faiss-cpu와 sentence-transformers 패키지를 설치합니다. \n",
    "FAISS는 CPU만 사용하는 버전(faiss-cpu)과 GPU를 지원하는 버전(faiss-gpu)으로 나뉘는데, 여기서는 CPU 버전을 설치하는 방법으로 설명합니다. sentence-transformers는 임베딩 모델을 허깅페이스에서 다운로드 받기 위해서 설치합니다.\n",
    "\n",
    "```\n",
    "pip install faiss-cpu sentence-transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "789ad21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x17e66b090>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터스토어 db 인스턴스를 생성\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sbert-nli',\n",
    "    model_kwargs={'device':'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")\n",
    "# HuggingFaceEmbeddings 클래스를 사용하여 사전 학습된 임베딩 모델(jhgan/ko-sbert-nli)을 로드하고, FAISS.from_documents 메서드를 사용하여 문서 객체를 임베딩 벡터로 변환하여 벡터 저장소에 저장합니다\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents,\n",
    "                                   embedding = embeddings_model,\n",
    "                                   distance_strategy = DistanceStrategy.COSINE\n",
    "                                  )\n",
    "vectorstore\n",
    "\n",
    "# 단, 문서 객체를 임베딩 벡터로 변환하여 벡터 저장소에 저장할 때, 모델의 입력 길이 제한을 고려해야 합니다. \n",
    "# jhgan/ko-sbert-nli 모델의 경우 최대 시퀀스 길이는 128 토큰입니다. (일반적인 BERT 기반 모델은 최대 시퀀스 길이가 512 토큰입니다.) \n",
    "# 최대 시퀀스 길이를 초과하는 입력 문장은 잘리거나 패딩 처리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7028c5",
   "metadata": {},
   "source": [
    "## RAG - Retriever\n",
    "Retrieval Augmented Generation (RAG)에서 검색도구(Retrievers)는 벡터 저장소에서 문서를 검색하는 핵심 구성 요소입니다. \n",
    "LangChain은 간단한 의미 검색부터 성능 향상을 위한 다양한 고급 검색 알고리즘을 지원합니다.\n",
    "\n",
    "RAG 시스템의 검색 품질은 최종 답변의 정확도에 직접적인 영향을 미칩니다. \n",
    "LangChain은 다양한 검색 전략을 제공하여 사용 사례에 맞는 최적의 검색 방식을 선택할 수 있습니다.\n",
    "\n",
    "\n",
    "|기법|핵심 원리|장점|적합한 상황|\n",
    "|------|---|----|----|\n",
    "|Vector Store Retriever|벡터 유사도 검색|간단, 빠름|기본 RAG 구현|\n",
    "|Multi Query Retriever|다중 쿼리 생성|검색 범위 확장|모호한 질문 처리|\n",
    "|Contextual Compression|관련 내용만 추출|노이즈 제거, 비용 절감|긴 문서에서 핵심 추출|\n",
    "|Ensemble Retriever|BM25 + 벡터 결합|키워드와 의미 검색 통합|전문 용어가 중요한 도메인|\n",
    "|RAG-Fusion|RRF로 결과 병합|다양성과 관련성 균형|복잡한 질문|\n",
    "|Decomposition|질문 분해|복잡한 질문 처리|다단계 추론 필요|\n",
    "|Step Back|추상화 질문 생성|배경 지식 활용|구체적 세부사항 질문|\n",
    "|HyDE|가상 문서 임베딩|질문-문서 갭 해소|짧은 질문, 긴 문서|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abfe8f",
   "metadata": {},
   "source": [
    "- 검색 기법 선택 가이드\n",
    "    기본 검색\n",
    "    - Vector Store Retriever\n",
    "      - 모든 RAG 시스템의 기본. 단순하고 빠른 검색이 필요할 때\n",
    "    - Multi Query Retriever\n",
    "      - 사용자 질문이 다양하게 해석될 수 있을 때\n",
    "    - Contextual Compression\n",
    "      - 검색된 문서가 길고 관련 없는 내용이 많을 때\n",
    "\n",
    "    고급 검색\n",
    "    - Ensemble Retriever\n",
    "        -  키워드 매칭이 중요한 기술/법률/의료 도메인\n",
    "    - RAG-Fusion\n",
    "        - 검색 결과의 다양성과 정확도를 모두 높이고 싶을 때\n",
    "    - Decomposition\n",
    "        - \"A와 B의 차이점은?\" 같은 복합 질문 처리\n",
    "    - Step Back\n",
    "        - \"2023년 삼성전자 반도체 매출은?\" 같이 구체적 사실 질문\n",
    "    - HyDE\n",
    "        - 짧은 질문으로 긴 문서를 검색할 때 (질문-문서 임베딩 불일치 해소)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
